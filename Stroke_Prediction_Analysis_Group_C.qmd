---
title: "Stroke_Prediction_Analysis_Group_C"
format: html
runtime: shiny
editor: visual
execute:
  cache: false
---

## 1. Introduction and Data Structure

### 1.1 Project Goals
Describe the main goals of the project, including the motivation behind the research and the key questions you aim to answer.

### 1.2 Research Questions
- Question 1: ...
- Question 2: ...
- (Add as necessary)

### 1.3 Adjustments or Refinements
Discuss any changes or refinements to the project scope or questions based on initial findings or data constraints.

### 1.4 Data Source

### 1.5 Data Import

```{r}
library(tidyverse)
library(knitr)
library(naniar)
```

You can add options to executable code like this

```{r}
#import of the dataset
stroke_db <- read.csv("data/stroke_dataset.csv")

#saving as a tibble
stroke_tb <- as_tibble(stroke_db)
head(stroke_tb)
```

In total, there are 12 variables including our binary target variable `stroke`.

## 2. Data Cleaning & Wrangeling

### 2.1 Data Exploration

Let's get a feeling for the distribution of our data:

```{r}
# Generate summary of your data frame
summary_table <- summary(stroke_tb)

# Convert the summary to a data frame & display it with kable
summary_table <- as.data.frame.matrix(summary_table)
kable(as.data.frame(summary_table), row.names = FALSE)

```

Our Dataset has a few categorical variables. As a machine learning model needs numerical inputs, we need to convert and encode them later. Let's create a list of categorical and numerical columns:

```{r}

categorical_variables <- names(stroke_tb)[sapply(stroke_tb,is.character)]
numerical_variables <- names(stroke_tb)[sapply(stroke_tb, is.numeric)]

# Use sprintf to format the message, then cat to print it with line breaks
cat(sprintf("Categorical features: %s\nNumerical features: %s",
            paste(categorical_variables, collapse = ", "),
            paste(numerical_variables, collapse = ", ")))
```

### 2.2 Missing Values

Let's look specifically at the missing values in our dataset:

```{r}
stroke_tb %>%
  summarize_all(~ sum(is.na(.) | . %in% c("N/A", "Unknown", "-")))
```

The dataset is fairly clean. Only the the features `bmi` and `smoking_status` have missing values. Let's change all missing values to N/A and ensure that `bmi` is recognized as a numerical variable. Additionally, the `id` column will be removed, because it is of no use for the future.

```{r}
cleaned_stroke_tb <- replace_with_na(data = stroke_tb, 
                                     replace = list(bmi = c("N/A"), smoking_status = c("Unknown"))) %>%
  mutate(bmi = as.numeric(bmi)) %>%
  select(-id)

cleaned_stroke_tb

write.csv(cleaned_stroke_tb, "data/cleaned_stroke_tb.csv", row.names = FALSE)

```

We update the lists of categorical and numerical columns, as `bmi` is now a double. !!!

```{r}
# Selecting categorical variables (character or factor types)
categorical_variables_1 <- names(cleaned_stroke_tb)[sapply(cleaned_stroke_tb, function(x) is.character(x) | is.factor(x))]

# Selecting numerical variables (double type)
numerical_variables <- names(cleaned_stroke_tb)[sapply(cleaned_stroke_tb, is.double)]


# Use sprintf to format the message, then cat to print it with line breaks
cat(sprintf("Categorical features: %s\nNumerical features: %s",
            paste(categorical_variables_1, collapse = ", "),
            paste(numerical_variables, collapse = ", ")))
```

As we would like to keep as many observations as possible in our dataset, we replace the N/As with the median for `bmi` and the modal value for `smoking_status`.

```{r}
# Calculate the median for bmi and the mode for smoking_status
bmi_median <- median(cleaned_stroke_tb$bmi, na.rm = TRUE)
smoking_status_mode <- names(which.max(table(cleaned_stroke_tb$smoking_status, useNA = "no")))

# Create preprocessed data by replacing NA values
preprocessed_stroke_tb <- cleaned_stroke_tb %>%
  mutate(
    bmi = ifelse(is.na(bmi), bmi_median, bmi),
    smoking_status = ifelse(is.na(smoking_status), smoking_status_mode, smoking_status)
  )

preprocessed_stroke_tb

# Save the modified dataset as a CSV file in the 'data' folder
write.csv(preprocessed_stroke_tb, "data/preprocessed_stroke_tb.csv", row.names = FALSE)

```

### 2.3 Categorical Feature Encoding

To make use of our categorical variables in our predictive model, we need to convert them into dummy variables. We use the `fastDummies` package for the One Hot Encoding.

```{r}
library(fastDummies)

encoded_stroke_tb <- dummy_cols(preprocessed_stroke_tb, select_columns=categorical_variables_1, remove_selected_columns = TRUE)

encoded_stroke_tb

# Save the modified dataset as a CSV file in the 'data' folder
write.csv(encoded_stroke_tb, "data/encoded_stroke_tb.csv", row.names = FALSE)

```

Multicollinearity between the different features will be a problem for our model. It will be treated in the Correlation Analysis.

```{r}
categorical_variables <- names(encoded_stroke_tb)[sapply(encoded_stroke_tb,is.integer)]
numerical_variables <- names(encoded_stroke_tb)[sapply(encoded_stroke_tb, is.double)]

# Use sprintf to format the message, then cat to print it with line breaks
cat(sprintf("Categorical features: %s\nNumerical features: %s",
            paste(categorical_variables, collapse = ", "),
            paste(numerical_variables, collapse = ", ")))
```

### 2.4 Outlier Analysis

Looking at the encoded stroke dataset for the categorical variables, we notice that there are only 3 non-binary quantitative variables : age, average glucose level and bmi. To visualize outliers in each of these variables, we use boxplots. The points outside the range delimited by the whiskers are the outliers.

```{r}
# Boxplot for Age
boxplot(encoded_stroke_tb$age, main="Boxplot of Age", ylab="Age", col="lightblue")

# Boxplot for Average Glucose Level
boxplot(encoded_stroke_tb$avg_glucose_level, main="Boxplot of Average Glucose Level", ylab="Average Glucose Level", col="lightgreen")

# Boxplot for BMI
boxplot(encoded_stroke_tb$bmi, main="Boxplot of BMI", ylab="BMI", col="lightcoral")

#We use the IQR method to identify outliers
identify_outliers <- function(column) {
  Q1 <- quantile(column, 0.25)
  Q3 <- quantile(column, 0.75)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Identify Outliers
  outliers <- column[column < lower_bound | column > upper_bound]
  return(outliers)
}

# Apply the function to the variables
outliers_age <- identify_outliers(encoded_stroke_tb$age)
outliers_glucose <- identify_outliers(encoded_stroke_tb$avg_glucose_level)
outliers_bmi <- identify_outliers(encoded_stroke_tb$bmi)

#We create a function count_outliers that we will use to identify and count outliers in each non-binary quantitative variable 
  count_outliers <- function(column) {
  Q1 <- quantile(column, 0.25, na.rm = TRUE)
  Q3 <- quantile(column, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
# Count outliers
outlier_count <- sum(column < lower_bound | column > upper_bound, na.rm = TRUE)
  return(outlier_count)
}

# Create a vector to hold the outlier counts
outlier_counts <- sapply(encoded_stroke_tb[, c("age", "avg_glucose_level", "bmi")], count_outliers)

```

Finding zero outliers for age indicates a relatively normative distribution among stroke patients, suggesting accurate and reliable data. With a dataset containing 5,110 rows, the identification of 627 outliers for average glucose levels, representing approximately 12.25% of the total population, and 126 outliers for Body Mass Index (BMI), constituting about 2.47%, highlights disparities in the distribution of these health metrics. The notably higher percentage of glucose level outliers suggests a prevalence of extreme values that may indicate issues with glycemic control among individuals in this population. The presence of a relatively significant number of outliers, particularly in average glucose levels, can skew the dataset's overall mean and standard deviation, potentially leading to misleading interpretations. These outliers may disproportionately influence statistical models, such as the regression analyses, affecting outcomes and relationships. Hence, it will be essential to assess the impact of these outliers on model performance and prioritize careful handling, whether through removal, transformation, or separate analyses, to ensure robust conclusions and accurate insights. The presence of many outliers can sometimes point to measurement errors or inconsistencies in data collection.

To have a better view on the distribution of values, we can also create histograms.

```{r}
hist(encoded_stroke_tb$age, breaks=30, main="Histogram of Age", xlab="Age", col="lightblue", border="black")

# Histogram for Average Glucose Level
hist(encoded_stroke_tb$avg_glucose_level, breaks=30, main="Histogram of Average Glucose Level", xlab="Average Glucose Level", col="lightgreen", border="black")

# Histogram for BMI
hist(encoded_stroke_tb$bmi, breaks=30, main="Histogram of BMI", xlab="BMI", col="lightcoral", border="black")
```

Now that we have conducted the analysis of the non-binary variables, we check the distribution of the binary variables.

```{r}
# Distribution of Gender Male
table(encoded_stroke_tb$gender_Male)
ggplot(encoded_stroke_tb, aes(x=factor(gender_Male))) + 
  geom_bar() + 
  labs(x='1 if gender is Male, 0 if not', y='Count', title='Distribution of Male gender')

# Distribution of Gender Female
table(encoded_stroke_tb$gender_Female)
ggplot(encoded_stroke_tb, aes(x=factor(gender_Female))) + 
  geom_bar() + 
  labs(x='1 if gender is Female, 0 if not', y='Count', title='Distribution of Female gender')

# Distribution of Gender Other
table(encoded_stroke_tb$gender_Other)
ggplot(encoded_stroke_tb, aes(x=factor(gender_Other))) + 
  geom_bar() + 
  labs(x='1 if gender is Other, 0 if not', y='Count', title='Distribution of Other gender')

# Distribution of Ever Married 
table(encoded_stroke_tb$ever_married_Yes)
ggplot(encoded_stroke_tb, aes(x=factor(ever_married_Yes))) + 
  geom_bar() + 
  labs(x='1 if Ever Married, 0 if not', y='Count', title='Distribution of  Ever Married Status')

# Distribution of Never Married 
table(encoded_stroke_tb$ever_married_No)
ggplot(encoded_stroke_tb, aes(x=factor(ever_married_No))) + 
  geom_bar() + 
  labs(x='1 if Never Married, 0 if not', y='Count', title='Distribution of Never Married Status')

# Distribution of Work Type is Children
table(encoded_stroke_tb$work_type_children)
ggplot(encoded_stroke_tb, aes(x=factor(work_type_children))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Children, 0 if not', y='Count', title='Distribution of Work Type is Children')

# Distribution of Work Type is Government job
table(encoded_stroke_tb$work_type_Govt_job)
ggplot(encoded_stroke_tb, aes(x=factor(work_type_Govt_job))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Government Job, 0 if not', y='Count', title='Distribution of Work Type is Government Job')

# Distribution of Work Type is Never worked
table(encoded_stroke_tb$work_type_Never_worked)
ggplot(encoded_stroke_tb, aes(x=factor(work_type_Never_worked))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Never Worked, 0 if not', y='Count', title='Distribution of Work Type is Never worked')

# Distribution of Work Type is Private
table(encoded_stroke_tb$work_type_Private)
ggplot(encoded_stroke_tb, aes(x=factor(work_type_Private))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Private, 0 if not', y='Count', title='Distribution of Work Type is Private')

# Distribution of Work Type is Self-empployed
table(encoded_stroke_tb$`work_type_Self-employed`)
ggplot(encoded_stroke_tb, aes(x=factor(`work_type_Self-employed`))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Self-employed, 0 if not', y='Count', title='Distribution of Work Type is Self-employed')

# Distribution of Residence Type is Rural
table(encoded_stroke_tb$Residence_type_Rural)
ggplot(encoded_stroke_tb, aes(x=factor(`Residence_type_Rural`))) + 
  geom_bar() + 
  labs(x='1 if Residence Type is Rural, 0 if not', y='Count', title='Distribution of Residence Type is Rural')

# Distribution of Residence Type is Urban
table(encoded_stroke_tb$Residence_type_Urban)
ggplot(encoded_stroke_tb, aes(x=factor(`Residence_type_Urban`))) + 
  geom_bar() + 
  labs(x='1 if Residence Type is Urban, 0 if not', y='Count', title='Distribution of Residence Type is Urban')

# Distribution of Formerly Smoked Status
table(encoded_stroke_tb$`smoking_status_formerly smoked`)
ggplot(encoded_stroke_tb, aes(x=factor(`smoking_status_formerly smoked`))) + 
  geom_bar() + 
  labs(x='1 if Formerly Smoked, 0 if not', y='Count', title='Distribution of Formerly Smoked Status')

# Distribution of Never Smoked Status
table(encoded_stroke_tb$`smoking_status_never smoked`)
ggplot(encoded_stroke_tb, aes(x=factor(`smoking_status_never smoked`))) + 
  geom_bar() + 
  labs(x='1 if Never Smoked, 0 if not', y='Count', title='Distribution of Never Smoked Status')

# Distribution of Smokes Status
table(encoded_stroke_tb$smoking_status_smokes)
ggplot(encoded_stroke_tb, aes(x=factor(`smoking_status_smokes`))) + 
  geom_bar() + 
  labs(x='1 if Smokes, 0 if not', y='Count', title='Distribution of Smokes Status')


```

Looking at the barplot for gender is "Other", it seems that there is no case where gender is "Other" in the dataset. However, by executing the line of code below, we notice that there is one case where gender is "Other".

```{r}
count_other_gender <- sum(encoded_stroke_tb$gender_Other)
count_other_gender
```

As it is only present once in all the dataset (5'110 obeservations), we decide to delete this variable.

To find on which line of the dataset one can find the individual in question, we use :

```{r}
# Find the row indices for individuals where gender is "Other"
other_gender_indices <- which(encoded_stroke_tb$gender_Other == 1)
other_gender_indices
# View the details of the individuals where gender is "Other"
encoded_stroke_tb[other_gender_indices, ]
```

Another plot showing an interesting pattern is the distribution of "Work Type is Never worked". The low representation (22 observations) of this former category may suggest that this group is under-represented, which can often lead to instability in outlier identification and might necessitate further scrutiny of their characteristics. We also decide to remove this variable from our analysis.

```{r}
count_work_type_Never_worked <- sum(encoded_stroke_tb$work_type_Never_worked)
count_work_type_Never_worked
```

The distribution of residence type between rural and urban appears to be quiet equitable and can provide valuable insights into demographic representation.

### 2.5 Dataset Balancing

```{r}

# Distribution of strokes vs no strokes in the sample
table(encoded_stroke_tb$stroke)
ggplot(encoded_stroke_tb, aes(x=factor(`stroke`))) + 
  geom_bar() + 
  labs(x='0 no strokes, 1 strokes', y='Count', title='Distribution of strokes')


# count how many people had a stroke and the prop
encoded_stroke_tb %>%
  group_by(stroke) %>%
  summarize(count = n()) %>%
  mutate(proportion = round(count / sum(count), 2))

```

The dataset shows that most individuals do not have strokes. However, such an imbalance can negatively impact model performance, as models may become biased towards the majority class, leading to poor prediction accuracy for minority cases, such as strokes. To address this, we can use dataset balancing techniques to adjust the proportions of stroke and no-stroke cases. This can be done through down-sampling (reducing the number of no-stroke cases) or up-sampling (increasing the number of stroke cases).

Since the dataset contains only 5,110 individuals, we choose to up-sample the minority class to avoid losing valuable data that could be discarded in down-sampling. By up-sampling, we can enhance the representation of stroke cases, which helps the model learn more effectively and improve prediction accuracy for both classes.

```{r}
# Load caret library
library(caret)

# Convert 'stroke' to a factor, if it isn't already
encoded_stroke_tb$stroke <- as.factor(encoded_stroke_tb$stroke)

# Use upSample to balance the dataset
balanced_stroke_tb <- upSample(x = encoded_stroke_tb[, names(encoded_stroke_tb) != "stroke"],
                               y = encoded_stroke_tb$stroke,
                               yname = "stroke")

# Check the distribution to confirm balancing
table(balanced_stroke_tb$stroke)

# Save the modified dataset as a CSV file in the 'data' folder
write.csv(balanced_stroke_tb, "data/balanced_stroke_tb.csv", row.names = FALSE)

```

Are the proportions of variables such as age, gender, and hypertension in the up-sampled stroke cases similar to those in the original stroke cases?

When we up-sample the minority class (stroke cases), we create artificial (or duplicated) rows to balance the dataset. In most basic up-sampling methods, like simple random sampling with replacement, these additional rows are duplicates of existing stroke cases. As a result, the distribution of other variables/columns (such as age, BMI, etc.) among stroke cases in the balanced dataset will match the original proportions found in the smaller stroke class of the initial dataset.

# Short comparison of the original and balanced dataset

## 3. Exploratory Data Analysis

### 3.1 Descriptive Statistics


```{r}
# Generate summary of your data frame
summary_table <- summary(stroke_tb)

# Convert the summary to a data frame & display it with kable
summary_table <- as.data.frame.matrix(summary_table)
kable(as.data.frame(summary_table), row.names = FALSE)

```

5,111 patient records 11 numerical and categorical variables describing the patients. Collected attributes include gender, age, history of hypertension, heart disease status, marital status, occupation type, type of residential area, average glucose levels, body mass index (BMI), smoking status, and stroke occurrence. Among the patients, 249 had experienced a stroke, while 4,861 had not. Ages range from newborn to 84 years, with an average age of 43.23, and age distribution appears close to normal. The dataset comprises 2,994 female patients, 2,115 male patients, and 1 patient identified as other. Average age is 43.23 years, with a standard deviation of 22.61 years, and a range of 0 to 82 years. Distribution of age is right-skewed, with a peak around 70 years old. Average glucose level is 106.15 mg/dL, with a standard deviation of 45.28 mg/dL, and a range of 55.12 to 271.74 mg/dL. ... Tell a story with charts.

```{r}
# Histogram for age
ggplot(encoded_stroke_tb, aes(x = age)) + 
  geom_histogram(binwidth = 5, fill = "dark grey", color = "white") +
  labs(title = "Distribution of Age", x = "Age", y = "Frequency")
```

```{r}
# Boxplot for hypertension
ggplot(encoded_stroke_tb, aes(y = hypertension, x = factor(stroke))) + 
  geom_boxplot() +
  labs(title = "Hypertension by Stroke Outcome", x = "Stroke (0 = No, 1 = Yes)", y = "Hypertension")
```

Idea: A sankey chart that shows the relationship between stroke, and its potential risk factors -Age -hypertension -heart disease -smoking status

```{r}
# Create a summarized data frame for the Sankey diagram
library(networkD3)
library(dplyr)

# Summarizing the counts based on hypertension and stroke
flow_data_hypertension <- encoded_stroke_tb %>%
  group_by(hypertension, stroke) %>%
  summarise(count = n()) %>%
  ungroup()

# Summarizing the counts based on heart disease and stroke
flow_data_heart_disease <- encoded_stroke_tb %>%
  group_by(heart_disease, stroke) %>%
  summarise(count = n()) %>%
  ungroup()

# Combine the two data frames into one for the Sankey diagram
flow_data <- bind_rows(
  flow_data_hypertension %>% mutate(from = as.character(hypertension), to = ifelse(stroke == 1, "Stroke", "No Stroke")),
  flow_data_heart_disease %>% mutate(from = as.character(heart_disease), to = ifelse(stroke == 1, "Stroke", "No Stroke"))
)

# Prepare a unique list of nodes
nodes <- data.frame(name = unique(c(flow_data$from, flow_data$to)))
```

```{r}
# Create the links for the Sankey diagram
flow_data$IDsource <- match(flow_data$from, nodes$name) - 1  # R uses 0-indexing for nodes
flow_data$IDtarget <- match(flow_data$to, nodes$name) - 1

# Generate the Sankey diagram
sankeyNetwork(
  Links = flow_data,
  Nodes = nodes,
  Source = 'IDsource',
  Target = 'IDtarget',
  Value = 'count',
  NodeID = 'name',
  units = 'Count',
  fontSize = 12,
  nodeWidth = 30
)
```

```{r}
# distribution of bmi
ggplot(encoded_stroke_tb, aes(x = bmi)) +
geom_histogram() +
labs(title = "Distribution of BMI")
```

Right-skewed distribution

```{r}
ggplot(encoded_stroke_tb, aes(x = age, fill = as.factor(stroke))) +
  geom_histogram(position = "fill") +
  labs(fill = "STROKE")
```

Strokes occur more frequently among older individuals in the population. The distribution of stroke cases is higher in the 60-80 age range, with a peak around 70 years old. This aligns with the general understanding that strokes are more common among older individuals, as age is a significant risk factor for stroke. The distribution of non-stroke cases is more evenly spread across age groups, with a higher proportion of younger individuals. This suggests that age may be a key factor in predicting stroke occurrence in this dataset. percentage not count

```{r}
ggplot(encoded_stroke_tb , aes(x = age, fill = as.factor(stroke))) +
  geom_histogram(position = "fill") +
  labs(fill = "STROKE")+
  facet_wrap(~gender_Male)
```

We can observe that there are no recorded cases of strokes in males under the age of 40.

```{r}
ggplot(encoded_stroke_tb , aes(x = age, fill = as.factor(stroke))) +
  geom_histogram(position = "fill") +
  labs(fill = "STROKE")+
  facet_wrap(~gender_Female)
```

In contrast, there are multiple instances of strokes in females younger than 40, inclduing cases at ages 30, 15, and even below one year old.

```{r}
ggplot(encoded_stroke_tb, aes(x = hypertension, fill = as.factor(stroke))) +
  geom_bar(position="fill") +
  labs(fill = "STROKE")
```

Hypertension seems to be a risk factor for strokes, as the proportion of stroke cases is higher among individuals with hypertension. This aligns with medical knowledge that hypertension is a significant risk factor for strokes. The distribution of stroke cases is higher among individuals with hypertension, suggesting that hypertension may be a key predictor of stroke occurrence in this dataset.

x axis is wrong

```{r}
# Create a bar chart for hypertension and stroke, faceted where each panel represents a gender group.
ggplot(encoded_stroke_tb , aes(x = hypertension, fill = as.factor(stroke))) +
  geom_bar(position="fill") +
  labs(fill = "STROKE")+
  facet_wrap(~gender_Female, labeller = as_labeller(c("0" = "Non-Female", "1" = "Female")))
```

Use pie charts instead of bar charts.

```{r}
ggplot(encoded_stroke_tb, aes(x = heart_disease, fill = as.factor(stroke))) +
  geom_bar(position = "fill") +
  labs(fill = "STROKE")
```

-age as a risk factor for a stroke – 24.9% of the strokes contained in the dataset belong to individual in the 75-79 year old age group, while only 4.01% were for patients under 45 years old Hypertension, a common risk factor for strokes per stroke.org, was greatly more prevalent in older patients, with 82.7% of cases occurring in those over the age of 50. based on EDA, Age, Hypertension, Heart Disease, and average glucose level are the most indicative risk factors for predicting a stroke based on this data

```{r}
library(ggbeeswarm)
ggplot(data = your_data, aes(x = factor(stroke), y = age)) +
  geom_beeswarm() +
  labs(x = "Stroke (0 = No, 1 = Yes)", y = "Age")
```


### 3.2 Univariate Analysis

#### 3.2.1 Feature Distribution & Target Variable Analysis

In the first part of the univariate analysis, we examine how predictor variables are correlated with our target variable `stroke`. In order to interpret correlations with `stroke` and distributions of the features simultaneously we use violin plots.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

library(shiny)
library(plotly)
library(dplyr)
library(htmltools)

# Shiny app with separate display of the correlation coefficient
shinyApp(
  ui = fluidPage(
    selectInput("selected_variable", "Choose a Variable:", choices = numerical_variables),
    plotlyOutput("violin_plot"),
    htmlOutput("correlation_text")  # Separate output for the correlation coefficient
  ),
  server = function(input, output) {
    
    output$violin_plot <- renderPlotly({
      selected_variable <- input$selected_variable
      
      # Generate the violin plot
      plot <- plot_ly(encoded_stroke_tb, x = ~factor(stroke), y = ~as.numeric(encoded_stroke_tb[[selected_variable]]), type = "violin",
                      box = list(visible = TRUE),
                      meanline = list(visible = TRUE)) %>%
        layout(
          title = paste("Violin Plot of", selected_variable, "by Stroke"),
          xaxis = list(title = "Stroke"),
          yaxis = list(title = selected_variable)
        )
      
      # Berechne die Korrelation
      correlation <- cor(as.numeric(encoded_stroke_tb[[selected_variable]]), as.numeric(encoded_stroke_tb$stroke), method = "pearson")
      
      # Erstelle HTML-Content mit Plot und Korrelationskoeffizient
      html_content <- tagList(
        tags$h3("Violin Plot of", selected_variable, "by Stroke"),
        plot,
        tags$p(paste("Correlation Coefficient with Stroke:", round(correlation, 2)))
      )
      
      # Speichere das HTML als Datei
      save_html(html_content, "data/violin_plot_and_target.html")
      
      plot  # Gebe den Plot aus
    })
    
    # Render the correlation text below the plot
    output$correlation_text <- renderUI({
      selected_variable <- input$selected_variable
      correlation <- cor(as.numeric(encoded_stroke_tb[[selected_variable]]), as.numeric(encoded_stroke_tb$stroke), method = "pearson")
      HTML(paste("Correlation Coefficient with Stroke:", round(correlation, 2)))
    })
  }
)

```

`age`: Although age appears to be somewhat related to the likelihood of experiencing a stroke, the correlation coefficient of 0.25 suggests only a modest association. The violin plot shows that stroke occurrences are more concentrated among older individuals, particularly in the age range of 60 to 80, with a mean age around 68 for stroke cases. While age may contribute to stroke risk, the relatively low correlation suggests it is one of multiple factors influencing stroke, rather than a strong standalone predictor.

`avg_glucose_level`: The average glucose level has a weak correlation of 0.13 with stroke occurrence, indicating that while higher glucose levels are slightly more common among individuals who have experienced a stroke, the relationship is not strong. This aligns with medical knowledge, as elevated glucose levels are a known risk factor for stroke. However, in this dataset, the correlation is low, suggesting that glucose levels alone may not be a reliable predictor of stroke but could contribute when combined with other factors.

`bmi`: With a correlation coefficient of only 0.04, BMI shows virtually no relationship with stroke in this dataset. Both groups (stroke and non-stroke) have similar central tendencies and ranges in BMI, indicating that BMI does not appear to be a significant predictor of stroke in this analysis. This weak association suggests that BMI may not play an important role in differentiating between stroke and non-stroke cases.

Let's continue with the categorical variables and their relationship with the target variable `stroke`. Under the graphics we display further information about the association between the variables using the Chi-Square test and Cramér's V for effect size. Because Chi-square performs poorly on few instances, we remove in a first step outliers identified previously.

--> including all categorical variables

```{r}
# Removing all the outliers identified previously
outlier_stroke_tb <- subset(preprocessed_stroke_tb, gender != "Other" & work_type != "children" & !(work_type== "Never_worked"))



write.csv(outlier_stroke_tb, "data/outlier_stroke_tb.csv", row.names = FALSE)
```

```{r}
library(shiny)
library(plotly)
library(dplyr)
library(vcd)
library(htmltools)

cat_variables = c("hypertension", "heart_disease", "ever_married", "work_type", "Residence_type", "smoking_status")

# Shiny app with stacked bar chart and association test results
shinyApp(
  ui = fluidPage(
    selectInput("selected_variable", "Choose a Variable:", choices = cat_variables),
    plotlyOutput("stacked_bar_chart"),
    htmlOutput("test_results")  # Output for Chi-Square and Cramér's V results
  ),
  
  server = function(input, output) {
    
    output$stacked_bar_chart <- renderPlotly({
      selected_variable <- input$selected_variable
      
      # Generate the stacked bar chart data
      plot_data <- outlier_stroke_tb %>%
        count(stroke = as.factor(stroke), category = as.factor(!!sym(selected_variable))) %>%
        rename(count = n)
      
      # Create the stacked bar chart
      plot <- plot_ly(plot_data, x = ~stroke, y = ~count, color = ~category,
                      type = "bar") %>%
        layout(barmode = "stack",
               title = paste("Stacked Bar Chart of", selected_variable, "by Stroke"),
               xaxis = list(title = "Stroke"),
               yaxis = list(title = "Count"),
               legend = list(title = list(text = selected_variable)))
      
      # Speichere das Plot-Objekt, das in der App angezeigt wird
      plot
    })
    
    output$test_results <- renderUI({
      selected_variable <- input$selected_variable
      
      # Create a contingency table for stroke and the selected categorical variable
      contingency_table <- table(outlier_stroke_tb$stroke, outlier_stroke_tb[[selected_variable]])
      
      # Check if the contingency table has enough data for the chi-square test
      if (all(contingency_table > 0)) {
        # Run the chi-square test
        chi_square_result <- chisq.test(contingency_table)
        
        # Calculate Cramér's V to assess the strength of association
        cramers_v <- assocstats(contingency_table)$cramer
        
        # Display the test results
        test_results <- paste(
          "Chi-square P-value:", round(chi_square_result$p.value, 4), "<br>",
          "Cramér's V:", round(cramers_v, 4)
        )
      } else {
        test_results <- "Insufficient data for Chi-Square test"
      }
      
      # Speichere die Testresultate und den Plot gemeinsam als HTML
      html_content <- tagList(
        tags$h3(paste("Stacked Bar Chart of", selected_variable, "by Stroke")),
        plotlyOutput("stacked_bar_chart"),
        tags$p(HTML(test_results))
      )
      
      # Speichern als HTML-Datei
      save_html(html_content, "data/stacked_bar_chart_target.html")
      
      # Gebe die Testresultate in der App aus
      HTML(test_results)
    })
  }
)



```

Description of findings in stacked_bar_plot above

### 3.3 Bivariate Analysis

In our bivariate analysis, we examine the relationship between variables (here pairwise). For the numerical features we create a heatmap to display correlations:

```{r}
numeric_columns <- encoded_stroke_tb %>% 
  select(where(is.double))

# Calculate the correlation matrix
correlation_matrix <- cor(numeric_columns, use = "complete.obs")

# Convert the correlation matrix to a format that plotly can use
heatmap_data <- as.data.frame(as.table(correlation_matrix))

# Plot the correlation matrix as a heatmap
plot_ly(
  x = colnames(correlation_matrix),
  y = rownames(correlation_matrix),
  z = correlation_matrix,
  type = "heatmap",
  colorscale = "Viridis"  # Corrected typo
) %>%
  layout(
    title = "Correlation Matrix Heatmap",
    xaxis = list(title = "", tickangle = 45),
    yaxis = list(title = "")
  )
```

The results show that there exists a low to moderate correlation between our predictors. Overall we can conclude that the numerical features are relatively independent and have low predictive power on each other. In that sense, no multicollinearity is present and the predictors will add unique information to our model.

In the next step we analyze the relationship between categorical features using a Chi-square test between different columns using contingency tables. A Chi-square test allows us to detect associations between features.

```{r}
# Loop through each unique pair of categorical variables
for (index1 in 1:(length(categorical_variables_1) - 1)) {
  var1 <- categorical_variables_1[index1]
  
  for (index2 in (index1 + 1):length(categorical_variables_1)) {
    var2 <- categorical_variables_1[index2]
    
    # Create a contingency table
    contingency_table <- table(preprocessed_stroke_tb[[var1]], preprocessed_stroke_tb[[var2]])
    
    # Run the chi-square test
    chi_square_result <- chisq.test(contingency_table)
    
    # Print the result if it's significant
    if (chi_square_result$p.value < 0.05) {
      cat("Significant relationship between", var1, "and", var2, "\n")
      cat("P-value:", chi_square_result$p.value, "\n")
      print(contingency_table)
      cat("\n")
    }
  }
}

```

There seems to be an issue with outliers in our previous analysis, as a chi-square performs relatively poor on few instances. First we remove all outliers from the dataset and then use Cramér's V to test the effect size of the association between variables. We set a threshold of V=0.3 for a moderate magnitude effect for the following analysis:

```{r}
# Removing all the outliers identified previously
preprocessed_stroke_tb <- subset(preprocessed_stroke_tb, gender != "Other" & work_type != "children")
# Remove rows where work_type is "Never_worked" and smoking_status is either "smokes" or "formerly smoked"
preprocessed_stroke_tb <- preprocessed_stroke_tb[!(preprocessed_stroke_tb$work_type== "Never_worked" & preprocessed_stroke_tb$smoking_status %in% c("smokes", "formerly smoked")), ]

```

```{r}
# Load necessary library for Cramér's V
library(vcd)

# Loop through each unique pair of categorical variables
for (index1 in 1:(length(categorical_variables_1) - 1)) {
  var1 <- categorical_variables_1[index1]
  
  for (index2 in (index1 + 1):length(categorical_variables_1)) {
    var2 <- categorical_variables_1[index2]
    
    # Create a contingency table
    contingency_table <- table(preprocessed_stroke_tb[[var1]], preprocessed_stroke_tb[[var2]])
    
    # Check if the contingency table has enough data for the chi-square test
    if (all(contingency_table > 0)) {
      # Run the chi-square test
      chi_square_result <- chisq.test(contingency_table)
      
      # Calculate Cramér's V to assess the strength of association
      cramers_v <- assocstats(contingency_table)$cramer
      
      # Display the results for each pair
      cat("Relationship between", var1, "and", var2, "\n")
      cat("Chi-square P-value:", chi_square_result$p.value, "\n")
      cat("Cramér's V:", cramers_v, "\n")
      print(contingency_table)
      cat("\n")
    } else {
      cat("Skipping test for", var1, "and", var2, "due to insufficient data\n")
    }
  }
}
```

As no effect size appears to be greater than 0.3, we can safely conclude that our categorical features are sufficiently independent to use as predictors. In conclusion, multicollinearity does not appear to be present among the variables, and we can therefore proceed. We refrain from conducting a multivariate analysis, as the results from this section seem robust enough to also conclude no multicollinearity among multiple features.

### 3.4 Feature Scaling

Classification algorithms are often sensitive to scales. To prevent perturbations we standardize all continuous features in the dataset.

--\> finished balanced dataset as input :

```{r}
scaled_stroke_tb <- encoded_stroke_tb

#Scale only the double columns

scaled_stroke_tb[sapply(scaled_stroke_tb, is.double)] <- lapply(scaled_stroke_tb[sapply(scaled_stroke_tb, is.double)], scale)

scaled_stroke_tb <- as_tibble(scaled_stroke_tb)

scaled_stroke_tb
```

## 4. Predictive Model

### 4.1 Train-Test Split

### 4.2 Logistic Regression Model

## 5. Model Evaluation and Predictions

## 6. Conclusions
