---
title: "Stroke_Prediction_Analysis_Group_C"
format: html
editor: visual
execute:
  cache: true
---

## 1. Introduction and Data Structure

### 1.1 Overview & Motivation

### 1.2 Related Work

### 1.3 Research Questions

### 1.4 Data Source

### 1.5 Data Import

```{r}
library(tidyverse)
library(knitr)
library(naniar)
```

You can add options to executable code like this

```{r}
#import of the dataset
stroke_db <- read.csv("data/stroke_dataset.csv")

#saving as a tibble
stroke_tb <- as_tibble(stroke_db)
head(stroke_tb)
```

In total, there are 12 variables including our binary target variable `stroke`.

## 2. Data Cleaning & Wrangeling

### 2.1 Data Exploration

Let's get a feeling for the distribution of our data:

```{r}
# Generate summary of your data frame
summary_table <- summary(stroke_tb)

# Convert the summary to a data frame & display it with kable
summary_table <- as.data.frame.matrix(summary_table)
kable(as.data.frame(summary_table), row.names = FALSE)

```

Our Dataset has a few categorical variables. As a machine learning model needs numerical inputs, we need to convert and encode them later. Let's create a list of categorical and numerical columns:

```{r}

categorical_variables <- names(stroke_tb)[sapply(stroke_tb,is.character)]
numerical_variables <- names(stroke_tb)[sapply(stroke_tb, is.numeric)]

# Use sprintf to format the message, then cat to print it with line breaks
cat(sprintf("Categorical features: %s\nNumerical features: %s",
            paste(categorical_variables, collapse = ", "),
            paste(numerical_variables, collapse = ", ")))
```

### 2.2 Missing Values

Let's look specifically at the missing values in our dataset:

```{r}
stroke_tb %>%
  summarize_all(~ sum(is.na(.) | . %in% c("N/A", "Unknown", "-")))
```

The dataset is fairly clean. Only the the features `bmi` and `smoking_status` have missing values. Let's change all missing values to N/A and ensure that `bmi` is recognized as a numerical variable. Additionally, the `id` column will be removed, because it is of no use for the future.

```{r}
cleaned_stroke_tb <- replace_with_na(data = stroke_tb, 
                                     replace = list(bmi = c("N/A"), smoking_status = c("Unknown"))) %>%
  mutate(bmi = as.numeric(bmi)) %>%
  select(-id)

cleaned_stroke_tb
```

We update the lists of categorical and numerical columns, as \`bmi\` is now a double.

```{r}
categorical_variables <- names(cleaned_stroke_tb)[sapply(cleaned_stroke_tb,is.character)]
numerical_variables <- names(cleaned_stroke_tb)[sapply(cleaned_stroke_tb, is.numeric)]

# Use sprintf to format the message, then cat to print it with line breaks
cat(sprintf("Categorical features: %s\nNumerical features: %s",
            paste(categorical_variables, collapse = ", "),
            paste(numerical_variables, collapse = ", ")))
```

As we would like to keep as many observations as possible in our dataset, we replace the N/As with the median for `bmi` and the modal value for `smoking_status`.

```{r}
# Calculate the median for bmi and the mode for smoking_status
bmi_median <- median(cleaned_stroke_tb$bmi, na.rm = TRUE)
smoking_status_mode <- names(which.max(table(cleaned_stroke_tb$smoking_status, useNA = "no")))

# Create preprocessed data by replacing NA values
preprocessed_stroke_tb <- cleaned_stroke_tb %>%
  mutate(
    bmi = ifelse(is.na(bmi), bmi_median, bmi),
    smoking_status = ifelse(is.na(smoking_status), smoking_status_mode, smoking_status)
  )

preprocessed_stroke_tb
```

### 2.3 Categorical Feature Encoding

To make use of our categorical variables in our predictive model, we need to convert them into dummy variables. We use the \`fastDummies\` package for the One Hot Encoding.

```{r}
library(fastDummies)

encoded_stroke_tb <- dummy_cols(preprocessed_stroke_tb, select_columns=categorical_variables, remove_selected_columns = TRUE)

encoded_stroke_tb
```

Multicollinearity between the different features will be a problem for our model. It will be treated in the Correlation Analysis.

### 2.4 Feature Scaling

Classification algorithms are often sensitive to scales. To prevent perturbations we standardize all continuous features in the dataset.

```{r}
scaled_stroke_tb <- encoded_stroke_tb

# Scale only the double columns
scaled_stroke_tb[sapply(scaled_stroke_tb, is.double)] <- 
  lapply(scaled_stroke_tb[sapply(scaled_stroke_tb, is.double)], scale)

scaled_stroke_tb <- as_tibble(scaled_stroke_tb)

scaled_stroke_tb
```

### 2.5 Outlier Analysis

Taking the standardized dataset, we first analyze the non-binary quantitative variables. To identify outliers, we use boxplots. The points outside the range delimited by the whiskers are the outliers.

```{r}
# Boxplot for Age
boxplot(scaled_stroke_tb$age, main="Boxplot of Age", ylab="Age", col="lightblue")

# Boxplot for Average Glucose Level
boxplot(scaled_stroke_tb$avg_glucose_level, main="Boxplot of Average Glucose Level", ylab="Average Glucose Level", col="lightgreen")

# Boxplot for BMI
boxplot(scaled_stroke_tb$bmi, main="Boxplot of BMI", ylab="BMI", col="lightcoral")

#We use the IQR method to identify outliers
identify_outliers <- function(column) {
  Q1 <- quantile(column, 0.25)
  Q3 <- quantile(column, 0.75)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Identify Outliers
  outliers <- column[column < lower_bound | column > upper_bound]
  return(outliers)
}

# Apply the function to the variables
outliers_age <- identify_outliers(scaled_stroke_tb$age)
outliers_glucose <- identify_outliers(scaled_stroke_tb$avg_glucose_level)
outliers_bmi <- identify_outliers(scaled_stroke_tb$bmi)

# Display the results
print(outliers_age)
print(outliers_glucose)
print(outliers_bmi)

#We create a function count_outliers that we will use to identify and count outliers in each non-binary quantitative variable 
  count_outliers <- function(column) {
  Q1 <- quantile(column, 0.25, na.rm = TRUE)
  Q3 <- quantile(column, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
# Count outliers
outlier_count <- sum(column < lower_bound | column > upper_bound, na.rm = TRUE)
  return(outlier_count)
}

# Create a vector to hold the outlier counts
outlier_counts <- sapply(scaled_stroke_tb[, c("age", "avg_glucose_level", "bmi")], count_outliers)

# Display the outlier counts
outlier_counts
```

Finding zero outliers for age indicates a relatively normative distribution among stroke patients, suggesting accurate and reliable data. With a dataset containing 5,110 rows, the identification of 627 outliers for average glucose levels, representing approximately 12.25% of the total population, and 126 outliers for Body Mass Index (BMI), constituting about 2.47%, highlights disparities in the distribution of these health metrics. The notably higher percentage of glucose level outliers suggests a prevalence of extreme values that may indicate issues with glycemic control among individuals in this population. The presence of a relatively significant number of outliers, particularly in average glucose levels, can skew the dataset's overall mean and standard deviation, potentially leading to misleading interpretations. These outliers may disproportionately influence statistical models, such as the regression analyses, affecting outcomes and relationships. Hence, it will be essential to assess the impact of these outliers on model performance and prioritize careful handling, whether through removal, transformation, or separate analyses, to ensure robust conclusions and accurate insights. The presence of many outliers can sometimes point to measurement errors or inconsistencies in data collection.

To have a better view on the distribution of values, we can also create histograms.

```{r}
hist(scaled_stroke_tb$age, breaks=30, main="Histogram of Age", xlab="Age", col="lightblue", border="black")

# Histogram for Average Glucose Level
hist(scaled_stroke_tb$avg_glucose_level, breaks=30, main="Histogram of Average Glucose Level", xlab="Average Glucose Level", col="lightgreen", border="black")

# Histogram for BMI
hist(scaled_stroke_tb$bmi, breaks=30, main="Histogram of BMI", xlab="BMI", col="lightcoral", border="black")
```

Now that we have conducted the analysis of the non-binary variables, we check the distribution of the binary variables.

```{r}
# Distribution of Gender Male
table(scaled_stroke_tb$gender_Male)
ggplot(scaled_stroke_tb, aes(x=factor(gender_Male))) + 
  geom_bar() + 
  labs(x='1 if gender is Male, 0 if not', y='Count', title='Distribution of Male gender')

# Distribution of Gender Female
table(scaled_stroke_tb$gender_Female)
ggplot(scaled_stroke_tb, aes(x=factor(gender_Female))) + 
  geom_bar() + 
  labs(x='1 if gender is Female, 0 if not', y='Count', title='Distribution of Female gender')

# Distribution of Gender Other
table(scaled_stroke_tb$gender_Other)
ggplot(scaled_stroke_tb, aes(x=factor(gender_Other))) + 
  geom_bar() + 
  labs(x='1 if gender is Other, 0 if not', y='Count', title='Distribution of Other gender')

# Distribution of Ever Married 
table(scaled_stroke_tb$ever_married_Yes)
ggplot(scaled_stroke_tb, aes(x=factor(ever_married_Yes))) + 
  geom_bar() + 
  labs(x='1 if Ever Married, 0 if not', y='Count', title='Distribution of  Ever Married Status')

# Distribution of Never Married 
table(scaled_stroke_tb$ever_married_No)
ggplot(scaled_stroke_tb, aes(x=factor(ever_married_No))) + 
  geom_bar() + 
  labs(x='1 if Never Married, 0 if not', y='Count', title='Distribution of Never Married Status')

# Distribution of Work Type is Children
table(scaled_stroke_tb$work_type_children)
ggplot(scaled_stroke_tb, aes(x=factor(work_type_children))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Children, 0 if not', y='Count', title='Distribution of Work Type is Children')

# Distribution of Work Type is Government job
table(scaled_stroke_tb$work_type_Govt_job)
ggplot(scaled_stroke_tb, aes(x=factor(work_type_Govt_job))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Government Job, 0 if not', y='Count', title='Distribution of Work Type is Government Job')

# Distribution of Work Type is Never worked
table(scaled_stroke_tb$work_type_Never_worked)
ggplot(scaled_stroke_tb, aes(x=factor(work_type_Never_worked))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Never Worked, 0 if not', y='Count', title='Distribution of Work Type is Never worked')

# Distribution of Work Type is Private
table(scaled_stroke_tb$work_type_Private)
ggplot(scaled_stroke_tb, aes(x=factor(work_type_Private))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Private, 0 if not', y='Count', title='Distribution of Work Type is Private')

# Distribution of Work Type is Self-empployed
table(scaled_stroke_tb$`work_type_Self-employed`)
ggplot(scaled_stroke_tb, aes(x=factor(`work_type_Self-employed`))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Self-employed, 0 if not', y='Count', title='Distribution of Work Type is Self-employed')

# Distribution of Residence Type is Rural
table(scaled_stroke_tb$Residence_type_Rural)
ggplot(scaled_stroke_tb, aes(x=factor(`Residence_type_Rural`))) + 
  geom_bar() + 
  labs(x='1 if Residence Type is Rural, 0 if not', y='Count', title='Distribution of Residence Type is Rural')

# Distribution of Residence Type is Urban
table(scaled_stroke_tb$Residence_type_Urban)
ggplot(scaled_stroke_tb, aes(x=factor(`Residence_type_Urban`))) + 
  geom_bar() + 
  labs(x='1 if Residence Type is Urban, 0 if not', y='Count', title='Distribution of Residence Type is Urban')

# Distribution of Formerly Smoked Status
table(scaled_stroke_tb$`smoking_status_formerly smoked`)
ggplot(scaled_stroke_tb, aes(x=factor(`smoking_status_formerly smoked`))) + 
  geom_bar() + 
  labs(x='1 if Formerly Smoked, 0 if not', y='Count', title='Distribution of Formerly Smoked Status')

# Distribution of Never Smoked Status
table(scaled_stroke_tb$`smoking_status_never smoked`)
ggplot(scaled_stroke_tb, aes(x=factor(`smoking_status_never smoked`))) + 
  geom_bar() + 
  labs(x='1 if Never Smoked, 0 if not', y='Count', title='Distribution of Never Smoked Status')

# Distribution of Smokes Status
table(scaled_stroke_tb$smoking_status_smokes)
ggplot(scaled_stroke_tb, aes(x=factor(`smoking_status_smokes`))) + 
  geom_bar() + 
  labs(x='1 if Smokes, 0 if not', y='Count', title='Distribution of Smokes Status')


```

Looking at the barplot, it seems that there is no case where gender is "Other" in the dataset. However, by executing the line of code below, we notice that there is one case where gender is "Other".

```{r}
count_other_gender <- sum(scaled_stroke_tb$gender_Other)
count_other_gender
```

To find on which line of the dataset one can find the individual in question, we use :

```{r}
# Find the row indices for individuals where gender is "Other"
other_gender_indices <- which(scaled_stroke_tb$gender_Other == 1)
other_gender_indices
# View the details of the individuals where gender is "Other"
scaled_stroke_tb[other_gender_indices, ]
```

Another plot showing an interesting pattern is the distribution of "Work Type is Never worked". The low representation of this former category may suggest that this group is under-represented, which can often lead to instability in outlier identification and might necessitate further scrutiny of their characteristics.

The distribution of residence type between rural and urban appears to be quiet equitable and can provide valuable insights into demographic representation.

### 2.6 Dataset Balancing

```{r}

# Distribution of strokes vs no strokes in the sample
table(scaled_stroke_tb$stroke)
ggplot(scaled_stroke_tb, aes(x=factor(`stroke`))) + 
  geom_bar() + 
  labs(x='0 no strokes, 1 strokes', y='Count', title='Distribution of strokes')


# count how many people had a stroke and the prop
scaled_stroke_tb %>%
  group_by(stroke) %>%
  summarize(count = n()) %>%
  mutate(proportion = round(count / sum(count), 2))

```

The dataset shows that most individuals do not have strokes, aligning with trends in the general population. However, such an imbalance can negatively impact model performance, as models may become biased towards the majority class, leading to poor prediction accuracy for minority cases, such as strokes. To address this, we can use dataset balancing techniques to adjust the proportions of stroke and no-stroke cases. This can be done through down-sampling (reducing the number of no-stroke cases) or up-sampling (increasing the number of stroke cases).

Since the dataset contains only 5,110 individuals, I chose to up-sample the minority class to avoid losing valuable data that could be discarded in down-sampling. By up-sampling, we can enhance the representation of stroke cases, which helps the model learn more effectively and improve prediction accuracy for both classes.

```{r}
# Load caret library
library(caret)

# Convert 'stroke' to a factor, if it isn't already
scaled_stroke_tb$stroke <- as.factor(scaled_stroke_tb$stroke)

# Use upSample to balance the dataset
balanced_stroke_tb <- upSample(x = scaled_stroke_tb[, names(scaled_stroke_tb) != "stroke"],
                               y = scaled_stroke_tb$stroke,
                               yname = "stroke")

# Check the distribution to confirm balancing
table(balanced_stroke_tb$stroke)
```

Are the proportions of variables such as age, gender, and hypertension in the up-sampled stroke cases similar to those in the original stroke cases?

When we up-sample the minority class (stroke cases), we create artificial (or duplicated) rows to balance the dataset. In most basic up-sampling methods, like simple random sampling with replacement, these additional rows are duplicates of existing stroke cases. As a result, the distribution of other variables/columns (such as age, BMI, etc.) among stroke cases in the balanced dataset will match the original proportions found in the smaller stroke class of the initial dataset.

## 3. Exploratory Data Analysis

### 3.1 Descriptive Statistics

### 3.2 Data Visualization

### 3.3 Correlation Analysis

Multicollinearity

## 4. Predictive Model

## 5. Model Evaluation and Predictions

## 6. Conclusions
