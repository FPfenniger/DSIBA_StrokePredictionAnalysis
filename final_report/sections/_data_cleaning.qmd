---
title: "_data_cleaning"
format: html
server: shiny
---
# Data Cleaning & Wrangeling

The dataset is fairly clean. Our dataset contains several categorical variables. Since a machine learning model requires numerical inputs, we distinguish from the beginning categorical and numerical variables.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show code"
# identify categorical and numerical variables
categorical_variables <- names(stroke_tb)[sapply(stroke_tb, function(x) is.character(x) || is.integer(x))]
numerical_variables <- names(stroke_tb)[sapply(stroke_tb, is.double)]

# print
cat(sprintf("Categorical features: %s\nNumerical features: %s",
            paste(categorical_variables, collapse = ", "),
            paste(numerical_variables, collapse = ", ")))
```

## Missing Values
Secondly, we address missing values in the features `bmi` and `smoking_status` by replacing them with `N/A`. Additionally, we ensure that `bmi` is recognized as a numerical variable and update the lists of categorical and numerical columns, as `bmi` is now a double. The `id` column is also removed, as it is not relevant for our analysis.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show code"
cleaned_stroke_tb <- replace_with_na(data = stroke_tb, 
                                     replace = list(bmi = c("N/A"), smoking_status = c("Unknown"))) %>%
  mutate(bmi = as.numeric(bmi)) %>%
  select(-id)

head(cleaned_stroke_tb)

# Selecting categorical variables (character or factor types)
categorical_variables <- names(cleaned_stroke_tb)[sapply(cleaned_stroke_tb, function(x) is.character(x) || is.integer(x))]
numerical_variables <- names(cleaned_stroke_tb)[sapply(cleaned_stroke_tb, is.double)]


# Use sprintf to format the message, then cat to print it with line breaks
cat(sprintf("Categorical features: %s\nNumerical features: %s",
            paste(categorical_variables, collapse = ", "),
            paste(numerical_variables, collapse = ", ")))
```

Then, to maintain the number of observations in our dataset, we replace the `N/A` values in `bmi` with the median and in `smoking_status` with the modal value.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show code"
# Calculate the median for bmi and the mode for smoking_status
bmi_median <- median(cleaned_stroke_tb$bmi, na.rm = TRUE)
smoking_status_mode <- names(which.max(table(cleaned_stroke_tb$smoking_status, useNA = "no")))

# Create preprocessed data by replacing NA values
preprocessed_stroke_tb <- cleaned_stroke_tb %>%
  mutate(
    bmi = ifelse(is.na(bmi), bmi_median, bmi),
    smoking_status = ifelse(is.na(smoking_status), smoking_status_mode, smoking_status)
  )

head(preprocessed_stroke_tb)
```


## Categorical Feature Encoding
For the categorical feature encoding, we convert our categorical variables into dummy variables to utilize them in our predictive model.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show code"
encoded_stroke_tb <- preprocessed_stroke_tb

# Encoding binary categorical variables first
encoded_stroke_tb$ever_married <- ifelse(encoded_stroke_tb$ever_married == "Yes", 1, 0)
encoded_stroke_tb$Residence_type <- ifelse(encoded_stroke_tb$Residence_type == "Urban", 1, 0)

# Create dummy variables for the remaining character columns
to_encode_variables <- names(encoded_stroke_tb)[sapply(encoded_stroke_tb, is.character)]
encoded_stroke_tb <- dummy_cols(
  encoded_stroke_tb, 
  select_columns = to_encode_variables, 
  remove_selected_columns = TRUE
)

# View the final encoded dataset
head(encoded_stroke_tb)
```


Only the features `bmi` and `smoking_status` have missing values. As described previously, we replaced these missing values by `N/A` and swapped them with the median and modal value respectively. Additionally, `bmi` had been imported as a character, so we converted it to a numeric variable. The `id` column was also removed as it was not relevant for our analysis.

## Outlier Analysis

Upon examining the encoded stroke dataset for the categorical variables, we notice that there is only three non-binary quantitative variables: `age`, `avg_glucose_level` and `bmi`. To visualize outliers in each of these variables, we used boxplots. The points outside the range defined by the whiskers were identified as outliers.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show code"
# Boxplot for Age
boxplot(encoded_stroke_tb$age, main="Boxplot of Age", ylab="Age", col="lightblue")

# Boxplot for Average Glucose Level
boxplot(encoded_stroke_tb$avg_glucose_level, main="Boxplot of Average Glucose Level", ylab="Average Glucose Level", col="lightgreen")

# Boxplot for BMI
boxplot(encoded_stroke_tb$bmi, main="Boxplot of BMI", ylab="BMI", col="lightcoral")

#We use the IQR method to identify outliers
identify_outliers <- function(column) {
  Q1 <- quantile(column, 0.25)
  Q3 <- quantile(column, 0.75)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Identify Outliers
  outliers <- column[column < lower_bound | column > upper_bound]
  return(outliers)
}

# Apply the function to the variables
outliers_age <- identify_outliers(encoded_stroke_tb$age)
outliers_glucose <- identify_outliers(encoded_stroke_tb$avg_glucose_level)
outliers_bmi <- identify_outliers(encoded_stroke_tb$bmi)

#We create a function count_outliers that we will use to identify and count outliers in each non-binary quantitative variable 
  count_outliers <- function(column) {
  Q1 <- quantile(column, 0.25, na.rm = TRUE)
  Q3 <- quantile(column, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
# Count outliers
outlier_count <- sum(column < lower_bound | column > upper_bound, na.rm = TRUE)
  return(outlier_count)
}

# Create a vector to hold the outlier counts
outlier_counts <- sapply(encoded_stroke_tb[, c("age", "avg_glucose_level", "bmi")], count_outliers)
outlier_counts

```
Finding zero outliers for `age` indicates a nearly normal distribution among the individuals in our population. With a dataset containing 5,110 rows, the identification of 627 outliers for `avg_glucose_level`, representing approximately 12.25% of the total population, and 126 outliers for `bmi`, constituting about 2.47%. Both boxplots display a considerable number of outliers in the higher range, indicating that there are subsets of individuals in the dataset with unusually high glucose levels and BMI values. These outliers could be due to a variety of factors, such as lifestyle, health conditions, or measurement errors. Considering their importance as risk factors, we decided to keep them in the dataset for now. However, we may revisit this decision later in the analysis, especially as we didn't examine the distributions in the upsampled dataset yet.

After conducting the analysis of the numerical variables, we examine the distribution of the categorical variables.

Two bar plots captured our attention: the one for the feature `gender`=`Other` and the one for the `work_type`=`Never_worked`. The former appears only once in the dataset, while the latter appears just 22 times out of a total of 5,110 observations. Due to their extreme underrepresentation, we decided to exclude these observations from our analysis as they add unnecessary complexity.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show code"
# Distribution of Gender Other
table(encoded_stroke_tb$gender_Other)
ggplot(encoded_stroke_tb, aes(x=factor(gender_Other))) + 
  geom_bar() + 
  labs(x='1 if gender is Other, 0 if not', y='Count', title='Distribution of Other gender')

# Distribution of Work Type Never worked
table(encoded_stroke_tb$work_type_Never_worked)
ggplot(encoded_stroke_tb, aes(x=factor(work_type_Never_worked))) + 
  geom_bar() + 
  labs(x='1 if Work Type is Never Worked, 0 if not', y='Count', title='Distribution of Work Type is Never worked')

#Removal of the observations and columns
encoded_stroke_tb <- encoded_stroke_tb %>%
  filter(gender_Other == 0, work_type_Never_worked == 0)

encoded_stroke_tb <- encoded_stroke_tb %>%
  select(-gender_Other, -work_type_Never_worked)

# Verify the columns are removed
str(encoded_stroke_tb)
```

## Dataset Balancing

The dataset reveals that most individuals do not have strokes, creating a strong class imbalance. This imbalance can negatively impact model performance by biasing it towards the majority class, resulting in poor predictive accuracy for minority cases like strokes. Without applying upsampling, the model might overly favor "no stroke" predictions, yielding high overall accuracy but failing to identify actual stroke cases effectively (low recall). By upsampling the minority class, we ensure the model learns from stroke cases as well, improving its ability to detect strokes and offering a more balanced and meaningful predictive performance.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show the code"
# Plot the distribution of stroke cases
ggplot(encoded_stroke_tb, aes(x = factor(stroke))) + 
  geom_bar() + 
  labs(x = '0 = No Stroke, 1 = Stroke', y = 'Count', title = 'Distribution of Strokes')

# Count the number of people who had a stroke and calculate the proportion
encoded_stroke_tb %>%
  group_by(stroke) %>%
  summarize(count = n()) %>%
  mutate(proportion = round(count / sum(count), 2))
```
To address this, we used dataset balancing techniques to adjust the proportions of stroke and no-stroke cases. Given that the dataset contains only 5,110 individuals, we chose upsampling the minority class to retain all available data, which would otherwise be reduced through downsampling. By upsampling, we enhance the representation of stroke cases, helping the model learn more effectively and improve both prediction accuracy and recall, particularly for the minority class.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show the code"
# Convert 'stroke' to a factor
encoded_stroke_tb$stroke <- as.factor(encoded_stroke_tb$stroke)

# Use upSample to balance the dataset
balanced_stroke_tb <- upSample(
  x = encoded_stroke_tb[, names(encoded_stroke_tb) != "stroke"], # All columns except 'stroke'
  y = encoded_stroke_tb$stroke, # Target variable for upsampling
  yname = "stroke" # Column name for the target variable in the upsampled dataset
)

# Check the distribution to confirm balancing
table(balanced_stroke_tb$stroke)

```

## Feature scaling

Finally, as classification algorithms are often sensitive to scales and to prevent perturbations, we standardize all continuous features in the dataset.
```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show code"
scaled_stroke_tb <- balanced_stroke_tb

#Scale only the double columns

scaled_stroke_tb[sapply(scaled_stroke_tb, is.double)] <- lapply(scaled_stroke_tb[sapply(scaled_stroke_tb, is.double)], scale)

scaled_stroke_tb <- as_tibble(scaled_stroke_tb)

head(scaled_stroke_tb)