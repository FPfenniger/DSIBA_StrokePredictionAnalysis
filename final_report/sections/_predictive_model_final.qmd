---
title: "_predictive_model"
format: html
editor: 
  markdown: 
    wrap: 72
---

```{r}
library(caret)
library(tidyverse)
library(vcd)
library(knitr)
library(janitor)
library(caret)  
library(MLmetrics)  
library(pROC)
library(kableExtra)
```

# Final Data Preparations
 
```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
outlier_stroke_tb <- read.csv("../../data/datasets/outlier_stroke_tb.csv")
str(outlier_stroke_tb)
```

### Feature Encoding

For the categorical feature encoding, we convert our categorical
variables into dummy variables to utilize them in our predictive model.
To avoid the dummy variable trap, we drop the first level of each
categorical variable. We start by encoding binary variables as 0 and 1,
and then create dummy variables for the remaining categorical variables.
We use the `model.matrix` function to create dummy variables and drop
the first level to set a reference.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Initialize the dataset
encoded_stroke_tb <- outlier_stroke_tb

# Encode binary variables without phantom levels
encoded_stroke_tb$gender <- factor(
  ifelse(encoded_stroke_tb$gender == "Female", "1", 
         ifelse(encoded_stroke_tb$gender == "Male", "0", NA))
)

encoded_stroke_tb$ever_married <- factor(
  ifelse(encoded_stroke_tb$ever_married == "Yes", "1", 
         ifelse(encoded_stroke_tb$ever_married == "No", "0", NA))
)

encoded_stroke_tb$Residence_type <- factor(
  ifelse(encoded_stroke_tb$Residence_type == "Urban", "1", 
         ifelse(encoded_stroke_tb$Residence_type == "Rural", "0", NA))
)

```

In the next step, we encode the remaining categorical variables with multiple categories. Here we always define one reference category:
```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# smoking_status
smoking_status_matrix <- model.matrix(~ smoking_status - 1, data = outlier_stroke_tb)
colnames(smoking_status_matrix) <- gsub("smoking_status", "smoking_status", colnames(smoking_status_matrix))
# Exclude reference level "never smoked"
smoking_status_matrix <- smoking_status_matrix[, -grep("never smoked", colnames(smoking_status_matrix))]

# work_type
work_type_matrix <- model.matrix(~ work_type - 1, data = outlier_stroke_tb)
colnames(work_type_matrix) <- gsub("work_type", "work_type", colnames(work_type_matrix))
# Exclude reference level "Private"
work_type_matrix <- work_type_matrix[, -grep("Private", colnames(work_type_matrix))]

# Bind matrices to dataset
encoded_stroke_tb <- cbind(
  encoded_stroke_tb[, !(names(outlier_stroke_tb) %in% c("smoking_status", "work_type"))],
  smoking_status_matrix,
  work_type_matrix
)

str(encoded_stroke_tb)

```
And we turn the binary variables into factors with the correct levels and apply the `make.names()` function to the column names to ensure they are valid.
```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Ensure binary variables are factors with correct levels
binary_vars <- c("gender", "hypertension", "heart_disease", "ever_married", "Residence_type", "work_typechildren", "work_typeGovt_job", "work_typeSelf-employed", "stroke", "smoking_statusformerly smoked", "smoking_statussmokes")

encoded_stroke_tb[binary_vars] <- lapply(encoded_stroke_tb[binary_vars], function(col) {
  factor(col, levels = c("0", "1"))
})

# Apply make.names() to column names
colnames(encoded_stroke_tb) <- make.names(colnames(encoded_stroke_tb))

# Display the final dataset structure
kable(encoded_stroke_tb)

str(encoded_stroke_tb)
```

We set the reference level for `smoking_status` to "never smoked" and
for `work_type` to "Private". This way, we ensure that the model can
interpret the dummy variables correctly. To be sure that our model will
recognize the binary variables as such, we convert them to factors and
set the same levels for every categorical variable.

### Multicollinearity

To examine associations and multicollinearity between the categorical
variables, we use a chi-squared test. The chi-squared test is a
statistical test used to determine whether there is a significant
association between two categorical variables. To determine the
importance of the test result we additionally computed the Cramér's V
for effect size (threshold applied here 0.3). Additionally, we apply the
Fisher's Exact Test if there are zero counts in the contingency table.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

cat_variables = c("gender", "hypertension", "heart_disease", "ever_married", "Residence_type", "work_typechildren", "work_typeGovt_job", "work_typeSelf.employed", "smoking_statusformerly.smoked", "smoking_statussmokes")

for (index1 in 1:(length(cat_variables) - 1)) {
  var1 <- cat_variables[index1]
  
  for (index2 in (index1 + 1):length(cat_variables)) {
    var2 <- cat_variables[index2]
    
    contingency_table <- table(encoded_stroke_tb[[var1]], encoded_stroke_tb[[var2]])
    
    # Run Fisher's Exact Test if there are zero counts, otherwise Chi-square Test
    if (any(contingency_table == 0)) {
      chi_square_result <- fisher.test(contingency_table, simulate.p.value = TRUE)
    } else {
      chi_square_result <- chisq.test(contingency_table)
    }
    
    cramers_v <- assocstats(contingency_table)$cramer
    
    if (cramers_v > 0.3 && chi_square_result$p.value < 0.05) {
      cat("Relationship between", var1, "and", var2, "\n")
      cat("P-value:", chi_square_result$p.value, "\n")
      cat("Cramér's V:", cramers_v, "\n")
      print(contingency_table)
      cat("\n")
    }
  }
}
```

The only relationship, which seems to be problematic in terms of
multicollinearity seems to be between `ever_married` and
`work_typechildren`. This makes a lot of sense, as children are not yet married and therefore a strong correlation will consist between `ever_married` (0) and `work_typechildren` (1). For the predictive models, we will therefore exclude `work_typechildren` because evidence and our EDA suggests that stroke risk is correlated with age, making this variable a weak predictor. 

```{r}
#| code-fold: true
#| code-summary: "Click to show code"

# Drop work_typechildren
encoded_stroke_tb <- encoded_stroke_tb %>% 
  select(-work_typechildren)

str(encoded_stroke_tb)
```
As all other effect sizes appear to be smaller than 0.3, we can safely
conclude that all other categorical features are sufficiently
independent to use as predictors. In conclusion, multicollinearity does
not appear to be present among the variables, and we can therefore
proceed. The correlation matrix in the previous chapter indicated low
correlations between features as well. We therefore refrain from
conducting a multivariate analysis, as the results from this section
seem robust enough to also conclude no multicollinearity among multiple
features.

### Dataset Balancing

The dataset revealed that most individuals do not have strokes, creating
a strong class imbalance. This imbalance can negatively impact model
performance by biasing it towards the majority class, resulting in poor
predictive accuracy for minority cases like strokes. Without applying
upsampling, the model might overly favor "no stroke" predictions,
yielding high overall accuracy but failing to identify actual stroke
cases effectively (low recall). By upsampling the minority class, we
ensure the model learns from stroke cases as well, improving its ability
to detect strokes and offering a more balanced and meaningful predictive
performance.

Let's start by creating the two stratified datasets, where we take the
age of 60 to split our sample in two groups. As seen in our EDA,
individuals aged over 60 have a significantly higher risk of stroke.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show the code"

low_risk_age_tb <- encoded_stroke_tb %>%
  filter(age < 60)

high_risk_age_tb <- encoded_stroke_tb %>%
  filter(age >= 60)
```

Let's look at stroke occurrences in our three datasets:

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show the code"

# Count the number of people who had a stroke and calculate the proportion
encoded_stroke_tb %>%
  group_by(stroke) %>%
  summarize(count = n()) %>%
  mutate(proportion = round(count / sum(count), 2))

low_risk_age_tb %>%
  group_by(stroke) %>%
  summarize(count = n()) %>%
  mutate(proportion = round(count / sum(count), 2))

high_risk_age_tb %>%
  group_by(stroke) %>%
  summarize(count = n()) %>%
  mutate(proportion = round(count / sum(count), 2))
```

To address this issue, we use dataset balancing techniques to adjust the
proportions of stroke and no-stroke cases. Given that the dataset
contains only 5,110 individuals, we chose upsampling the minority class
to retain all available data, which would otherwise be reduced through
downsampling. By upsampling, we enhance the representation of stroke
cases, helping the model learn more effectively and improve both
prediction accuracy and recall, particularly for the minority class
(here when strokes occur).Because the upsampling in the low-risk age
group is rather extreme (from 1.5% to 50% stroke cases), we will use the
upsampled dataset only for training and use the original dataset for the
evaluation. We will therefore address the balancing and scaling part
later on, in the specific model part of the low-risk age group.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show the code"

# Use upSample to balance the dataset
balanced_stroke_tb <- upSample(
  x = encoded_stroke_tb[, names(encoded_stroke_tb) != "stroke"], 
  y = encoded_stroke_tb$stroke, 
  yname = "stroke"
)

high_risk_age_data <- upSample(
  x = high_risk_age_tb[, names(high_risk_age_tb) != "stroke"], 
  y = high_risk_age_tb$stroke, 
  yname = "stroke"
)


# Check the structure to confirm changes
str(balanced_stroke_tb)


# Check the distribution to confirm balancing
table(balanced_stroke_tb$stroke)

# Save the dataset
write.csv(balanced_stroke_tb, "../../data/datasets/balanced_stroke_tb.csv", row.names = FALSE)
```

### Scaling

Finally, as classification algorithms are often sensitive to feature
scales, we standardize all continuous features in the dataset to prevent
perturbations and improve model performance. Standardization rescales
features to have a mean of 0 and a standard deviation of 1. The
mathematical formula for standardization is as follows:

$$
z = \frac{x - \mu}{\sigma}
$$

Where: 
- z is the standardized value, 
- x is the original feature value, 
- mu is the mean of the feature, 
- sigma is the standard deviation of the feature.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Define a function to standardize continuous variables
standardize <- function(x) {
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}

# Apply standardization to continuous (numeric) columns in the dataset
scaled_stroke_tb <- balanced_stroke_tb %>%
  mutate(across(where(is.double), ~ standardize(.)))

scaled_unbalanced_stroke_tb <- encoded_stroke_tb %>%
  mutate(across(where(is.double), ~ standardize(.)))

scaled_high_risk_age_tb <- high_risk_age_data %>%
  mutate(across(where(is.double), ~ standardize(.)))

# Display the scaled dataset
kable(scaled_stroke_tb)

# Save the scaled datasets
write.csv(scaled_stroke_tb, "../../data/datasets/scaled_stroke_tb.csv", row.names = FALSE)
write.csv(scaled_unbalanced_stroke_tb, "../../data/datasets/scaled_unbalanced_stroke_tb.csv", row.names = FALSE)
write.csv(scaled_high_risk_age_tb, "../../data/datasets/scaled_high_risk_age_tb.csv", row.names = FALSE)
```
# Modelling

We divide our analysis into four different datasets and predictive
models: the original dataset, the balanced original dataset, the
low-risk age group, and the high-risk age group. We will train a
logistic regression model for each dataset, before evaluating them on
the test set. We will then compare the performance of the models and discuss potential strengths and weaknesses.

## Metrics and Terms
First of all we introduce the metrics we will be using to evaluate our models. 

- Confusion matrix
The confusion matrix compares the actual values of the target variable with the predicted values and serves as a basis for our evaluation. It is a 2x2 matrix that contains the following values: True Positives (predicting correctly a stroke), True Negatives (predicting correctly no stroke occurrence), False Positives (predicting a stroke when there is none) and False Negatives (predicting no stroke when there is one). The confusion matrix is used to calculate the following metrics:

- Precision
Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The formula for precision is given by:

$$
Precision = \frac{TP}{TP + FP}
$$
- Recall
Recall is the ratio of correctly predicted positive observations to the all observations in actual class. In our case and general in healthcare research, recall should be prioritized as undetected strokes are generally more fatal than false alarms. The formula for recall is given by: 

$$
Recall = \frac{TP}{TP + FN}
$$
- F1 Score
The F1 score is the harmonic mean of Precision and Recall and often used in unbalanced dataset like ours as it considers false positives and false negatives. The formula for F1 score ranges from 0 to 1 and is given by:
$$
F1 = 2×\frac{Precision × Recall}{Precision + Recall}
$$
- Specificity
Specificity is the ratio of correctly predicted negative observations to the total actual negative observations. The formula for specificity is given by:
$$
Specificity = \frac{TN}{TN + FP}
$$
- Accuracy
Accuracy is the ratio of correctly predicted observations to the total observations. The formula for accuracy is given by:
$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$

- ROC Curve and AUC
The *receiver operating characteristic* curve is a graphical representation of the true positive rate (recall) against the false positive rate (specificity). Concretley, It shows the tradeoff between sensitivity and specificity. The area under the curve (AUC) is a measure of how well the model can distinguish between classes. An AUC of 1 indicates a perfect model, while an AUC of 0.5 indicates a model that performs no better than random chance.

- Odds ratio
The odds ratio is the exponentiated coefficient of a logistic regression model. It represents the change in odds of the target variable for a one-unit change in the predictor variable. An odds ratio greater than 1 indicates a positive relationship between the predictor and the target variable, while an odds ratio less than 1 indicates a negative relationship. It will help us to understand the importance and direction of each feature in predicting the target variable.

## Baseline Model

We divide our analysis into four different datasets and predictive
models: the original dataset, the balanced original dataset, the
low-risk age group, and the high-risk age group. We will train a
logistic regression model for each dataset, before evaluating them on
the test set. We will then compare the performance of the models and discuss potential strengths and weaknesses.

### Train-test split

In order to evaluate the performance of our model, we split the original
dataset into a training and a test set. We use 80% of the data for
training and 20% for testing.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Load the dataset
unbalanced_original_data <- scaled_unbalanced_stroke_tb

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
unbalanced_original_data[cat_variables] <- lapply(unbalanced_original_data[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
unbalanced_original_data$stroke <- factor(
  unbalanced_original_data$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)

# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(unbalanced_original_data$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_unbalanced <- unbalanced_original_data[trainIndex, ]
test_unbalanced <- unbalanced_original_data[-trainIndex, ]

colnames(test_unbalanced) <- make.names(colnames(test_unbalanced))  # Make all column names valid
colnames(test_unbalanced) <- colnames(train_unbalanced)  # Align with the training dataset
```

### Model

With the `caret` package, we specify our first logistic model by
specifying (method= "glm") and (family = "binomial") to train a logistic
regression model.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      # No cross-validation
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_1 <- train(
  stroke ~ ., 
  data = train_unbalanced,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"


# Predict probabilities on the training set
train_predictions_prob <- predict(logistic_model_1, newdata = train_unbalanced, type = "prob")
train_predictions_class <- ifelse(train_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert predictions to factors to match the training set
train_predictions_class <- factor(train_predictions_class, levels = levels(train_unbalanced$stroke))

# Confusion Matrix for Training Set
conf_matrix_train <- confusionMatrix(train_predictions_class, train_unbalanced$stroke, positive = "Yes")

# Extract Metrics from the Confusion Matrix
precision <- conf_matrix_train$byClass["Precision"]
recall <- conf_matrix_train$byClass["Recall"]
f1_score <- conf_matrix_train$byClass["F1"]
specificity <- conf_matrix_train$byClass["Specificity"]
accuracy <- conf_matrix_train$overall["Accuracy"]

# Generate a Classification Report for the Training Set
classification_report_train <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)

```

The baseline model demonstrates a high overall accuracy of 95.08%, but
this is misleading due to its inability to effectively predict the
minority class (Yes). While specificity is nearly perfect at 99.97%,
indicating strong performance for the majority class (No), the model
struggles with recall for the positive class, identifying only 0.5% of
true positives. This results in a very low F1-score of 0.99%, reflecting
poor balance between precision and recall. These results highlight the
model's significant bias toward the majority class. For the evaluation on the test set we therefore expect exclusive prediction of "No" for the stroke variable and metrics in the same range as the training set.

### Evaluation and Discussion

We start by evaluating the model on the test.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_1, newdata = test_unbalanced, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_unbalanced$stroke <- factor(test_unbalanced$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_unbalanced$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}

# Check Confusion Matrix Input
print(table(test_predictions_class, test_unbalanced$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_unbalanced$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```
As previously stated, our baseline model didn't predict any strokes at all, which is reflected in the condfusion matrix. 
```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_unbalanced$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Step 1: Mapping function for comprehensive variable names
rename_variables <- function(variable) {
  case_when(
    variable == "age" ~ "Age",
    variable == "work_typeSelf.employed1" ~ "Work Type: Self-Employed",
    variable == "hypertension1" ~ "Hypertension",
    variable == "smoking_statussmokes1" ~ "Smoking Status: Smokes",
    variable == "work_typeGovt_job1" ~ "Work Type: Government Job",
    variable == "heart_disease1" ~ "Heart Disease",
    variable == "avg_glucose_level" ~ "Average Glucose Level",
    variable == "ever_married1" ~ "Ever Married",
    variable == "smoking_statusformerly.smoked1" ~ "Smoking Status: Formerly Smoked",
    variable == "gender1" ~ "Gender: Female",
    variable == "Residence_type1" ~ "Residence Type: Urban",
    variable == "bmi" ~ "BMI",
    TRUE ~ variable
  )
}


# Step 2: Define a function to generate the importance table for a specific model
generate_importance_table <- function(model, model_name = "Logistic Model") {
  # Extract coefficients
  coefficients <- summary(model)$coefficients
  
  # Create the variable importance table
  variable_importance <- as.data.frame(coefficients) %>%
    rownames_to_column("Predictor") %>%             # Convert row names to a column
    mutate(
      Predictor = rename_variables(Predictor),      # Rename variables
      `Odds Ratio` = exp(Estimate)                 # Calculate Odds Ratios
    ) %>%
    select(Predictor, Estimate, `Odds Ratio`, `Pr(>|z|)`) %>%  # Keep relevant columns
    rename(
      Coefficient = Estimate,                      # Rename Estimate to Coefficient
      `P-value` = `Pr(>|z|)`                       # Rename P-value column
    ) %>%
    filter(Predictor != "(Intercept)") %>%           # Exclude the intercept
    arrange(desc(`Odds Ratio`))                # Sort by absolute value of coefficients
  
  # Display the table with a dynamic caption
  kable(variable_importance, caption = paste(model_name, "- Predictor Importance with Odds Ratios")) %>%
    kable_styling(full_width = FALSE, position = "center")
}
generate_importance_table(logistic_model_1, "Logistic Model 1")
```

## Balanced Baseline Model

We repeat the same process for our balanced baseline model.

### Train-test split

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Load the dataset
balanced_original_data <- scaled_stroke_tb

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
balanced_original_data[cat_variables] <- lapply(balanced_original_data[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
balanced_original_data$stroke <- factor(
  balanced_original_data$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)

# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(balanced_original_data$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_balanced <- balanced_original_data[trainIndex, ]
test_balanced <- balanced_original_data[-trainIndex, ]
```

### Model

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      # No cross-validation
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_2 <- train(
  stroke ~ ., 
  data = train_balanced,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the training set
train_predictions_prob <- predict(logistic_model_2, newdata = train_balanced, type = "prob")
train_predictions_class <- ifelse(train_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert predictions to factors to match the training set
train_predictions_class <- factor(train_predictions_class, levels = levels(train_balanced$stroke))

# Confusion Matrix for Training Set
conf_matrix_train <- confusionMatrix(train_predictions_class, train_balanced$stroke, positive = "Yes")

# Extract Metrics from the Confusion Matrix
precision <- conf_matrix_train$byClass["Precision"]
recall <- conf_matrix_train$byClass["Recall"]
f1_score <- conf_matrix_train$byClass["F1"]
specificity <- conf_matrix_train$byClass["Specificity"]
accuracy <- conf_matrix_train$overall["Accuracy"]

# Generate a Classification Report for the Training Set
classification_report_train <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)
```

The balanced baseline model shows substantial improvement in handling
the minority class compared to the original baseline. With an overall
accuracy of 77.45%, the model strikes a better balance between the
positive and negative classes. Recall for the positive class is strong
at 81.35%, ensuring that most true positives are correctly identified.
The F1-Score of 78.29% reflects a good balance between precision and
recall. However, specificity remains relatively lower at 73.5%,
indicating some false positives. Overall, the model demonstrates
improved performance for both classes, but there is still room for
enhancing specificity and further reducing false positives.

### Evaluation and Discussion

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_2, newdata = test_balanced, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_balanced$stroke <- factor(test_balanced$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_balanced$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}

# Check Confusion Matrix Input
print(table(test_predictions_class, test_balanced$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_balanced$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_balanced$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

generate_importance_table(logistic_model_2, "Logistic Model 2")
```

## Low-Risk Age Model

In this section , we build our first stratified model for the low-risk
age group. Here we first split the original dataset into test and
training set and perform then the previously mentionned preprocessing
steps for the training set.

### Train-test split

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
low_risk_age_tb[cat_variables] <- lapply(low_risk_age_tb[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
low_risk_age_tb$stroke <- factor(
  low_risk_age_tb$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)


# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(low_risk_age_tb$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_low_risk <- low_risk_age_tb[trainIndex, ]
test_low_risk <- low_risk_age_tb[-trainIndex, ]

# Balance the training set 
train_low_risk <- upSample(
  x = train_low_risk[, names(train_low_risk) != "stroke"], 
  y = train_low_risk$stroke, 
  yname = "stroke"
) 

# Scale both training and test set
train_low_risk <- train_low_risk %>%
  mutate(across(where(is.double), ~ standardize(.)))

test_low_risk <- test_low_risk %>%
  mutate(across(where(is.double), ~ standardize(.)))
```

### Model

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_3 <- train(
  stroke ~ ., 
  data = train_low_risk,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the training set
train_predictions_prob <- predict(logistic_model_3, newdata = train_low_risk, type = "prob")
train_predictions_class <- ifelse(train_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert predictions to factors to match the training set
train_predictions_class <- factor(train_predictions_class, levels = levels(train_low_risk$stroke))

# Confusion Matrix for Training Set
conf_matrix_train <- confusionMatrix(train_predictions_class, train_low_risk$stroke, positive = "Yes")

# Extract Metrics from the Confusion Matrix
precision <- conf_matrix_train$byClass["Precision"]
recall <- conf_matrix_train$byClass["Recall"]
f1_score <- conf_matrix_train$byClass["F1"]
specificity <- conf_matrix_train$byClass["Specificity"]
accuracy <- conf_matrix_train$overall["Accuracy"]

# Generate a Classification Report for the Training Set
classification_report_train <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)

```


### Evaluation and Discussion

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_3, newdata = test_low_risk, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_low_risk$stroke <- factor(test_low_risk$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_low_risk$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}

# Check Confusion Matrix Input
print(table(test_predictions_class, test_low_risk$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_low_risk$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```
probably due to several factors:
- different stroke distribution in the training set
- overfitting to the training set structure
- the small size of the test set



```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_low_risk$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```


```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

generate_importance_table(logistic_model_3, "Logistic Model 3")
```

## High-Risk Age Model

### Train-test split

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"


# Load the dataset
high_risk_age_data <- scaled_high_risk_age_tb

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
high_risk_age_data[cat_variables] <- lapply(high_risk_age_data[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
high_risk_age_data$stroke <- factor(
  high_risk_age_data$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)

# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(high_risk_age_data$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_high_risk <- high_risk_age_data[trainIndex, ]
test_high_risk <- high_risk_age_data[-trainIndex, ]
```

### Model

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_4 <- train(
  stroke ~ ., 
  data = train_high_risk,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the training set
train_predictions_prob <- predict(logistic_model_4, newdata = train_high_risk, type = "prob")
train_predictions_class <- ifelse(train_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert predictions to factors to match the training set
train_predictions_class <- factor(train_predictions_class, levels = levels(train_high_risk$stroke))

# Confusion Matrix for Training Set
conf_matrix_train <- confusionMatrix(train_predictions_class, train_high_risk$stroke, positive = "Yes")

# Extract Metrics from the Confusion Matrix
precision <- conf_matrix_train$byClass["Precision"]
recall <- conf_matrix_train$byClass["Recall"]
f1_score <- conf_matrix_train$byClass["F1"]
specificity <- conf_matrix_train$byClass["Specificity"]
accuracy <- conf_matrix_train$overall["Accuracy"]

# Generate a Classification Report for the Training Set
classification_report_train <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)

```

The stratified model for the high-risk age group shows moderate
performance, with an overall accuracy of 63.70%. Recall for the positive
class (Yes) is 66.11%, indicating the model captures a fair portion of
true positives but leaves room for improvement. Precision is 63.07%,
meaning that about 63% of predicted positive cases are correct. The
F1-Score of 64.56% reflects a reasonable balance between precision and
recall, though not particularly strong. Specificity is low at 61.30%,
indicating the model struggles with correctly identifying negative
cases, leading to a higher false-positive rate. Overall, while the model
captures positive cases moderately well, its low specificity and overall
accuracy suggest there is significant room for improvement, especially
in distinguishing between the two classes.

### Evaluation and Discussion

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_4, newdata = test_high_risk, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_high_risk$stroke <- factor(test_high_risk$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_high_risk$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}
  
# Check Confusion Matrix Input
print(table(test_predictions_class, test_high_risk$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_high_risk$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
    Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_high_risk$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
generate_importance_table(logistic_model_4, "Logistic Model 4")
```

