---
title: "predictive_model"
format: html
editor: 
  markdown: 
    wrap: 72
---

```{r, message=FALSE, warning=FALSE}
library(caret)
library(tidyverse)
library(vcd)
library(knitr)
library(janitor)
library(caret)  
library(MLmetrics)  
library(pROC)
library(kableExtra)
```

# Final Data Preparation
In the following chapter we will prepare the data for the modeling phase. We will start by encoding the categorical variables, checking them for multicollinearity, balancing the dataset, and finally scaling the continuous variables (which is done after the train-test split in the Modeling part). 
```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Import of the dataset
outlier_stroke_tb <- read.csv("../../data/datasets/outlier_stroke_tb.csv")
# Structure of the dataset
str(outlier_stroke_tb)
```

### Feature Encoding

For the categorical feature encoding, we convert our categorical
variables into dummy variables to utilize them in our predictive model.
To avoid the dummy variable trap, we drop the first level of each
categorical variable and use it as a reference level. We start by encoding binary variables as 0 and 1.
```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Initialize the dataset
encoded_stroke_tb <- outlier_stroke_tb

# Encode binary variables
encoded_stroke_tb$gender <- factor(
  ifelse(encoded_stroke_tb$gender == "Female", "1", 
         ifelse(encoded_stroke_tb$gender == "Male", "0", NA))
)

encoded_stroke_tb$ever_married <- factor(
  ifelse(encoded_stroke_tb$ever_married == "Yes", "1", 
         ifelse(encoded_stroke_tb$ever_married == "No", "0", NA))
)

encoded_stroke_tb$Residence_type <- factor(
  ifelse(encoded_stroke_tb$Residence_type == "Urban", "1", 
         ifelse(encoded_stroke_tb$Residence_type == "Rural", "0", NA))
)
```

In the next step, we encode the remaining categorical variables `smoking_status` and `work_type` with
multiple categories. Here we always define one reference category. We use the `model.matrix()` function to create dummy variables and drop the first level. We set the reference level for `smoking_status` to "never smoked" and for `work_type` to "Private".
```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# smoking_status
smoking_status_matrix <- model.matrix(~ smoking_status - 1, data = outlier_stroke_tb)
colnames(smoking_status_matrix) <- gsub("smoking_status", "smoking_status", colnames(smoking_status_matrix))
# Exclude reference level "never smoked"
smoking_status_matrix <- smoking_status_matrix[, -grep("never smoked", colnames(smoking_status_matrix))]

# work_type
work_type_matrix <- model.matrix(~ work_type - 1, data = outlier_stroke_tb)
colnames(work_type_matrix) <- gsub("work_type", "work_type", colnames(work_type_matrix))
# Exclude reference level "Private"
work_type_matrix <- work_type_matrix[, -grep("Private", colnames(work_type_matrix))]

# Bind matrices to dataset
encoded_stroke_tb <- cbind(
  encoded_stroke_tb[, !(names(outlier_stroke_tb) %in% c("smoking_status", "work_type"))],
  smoking_status_matrix,
  work_type_matrix
)

str(encoded_stroke_tb)
```
And we turn the binary variables into factors with the correct levels
and apply the `make.names()` function to the column names to ensure they
are valid.
```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Ensure binary variables are factors with correct levels
binary_vars <- c("gender", "hypertension", "heart_disease", "ever_married", "Residence_type", "work_typechildren", "work_typeGovt_job", "work_typeSelf-employed", "stroke", "smoking_statusformerly smoked", "smoking_statussmokes")

encoded_stroke_tb[binary_vars] <- lapply(encoded_stroke_tb[binary_vars], function(col) {
  factor(col, levels = c("0", "1"))
})

# Apply make.names() to column names
colnames(encoded_stroke_tb) <- make.names(colnames(encoded_stroke_tb))

# Display the dataset with up to 10 observations per page
kable(encoded_stroke_tb, format = "html") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE) %>%
  scroll_box(height = "300px")

str(encoded_stroke_tb)
```

### Multicollinearity

To examine associations between the categorical variables, we use the **chi-squared test**. The chi-squared test is a statistical test used to determine whether there is a significant association between two categorical variables. The test statistic is computed as:

$$
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
$$

Where:
- \( O_i \) = Observed frequency
- \( E_i \) = Expected frequency

To determine the importance of the test result, we additionally compute **Cramér's V** for effect size, defined as:

$$
V = \sqrt{\frac{\chi^2}{n \cdot \text{min}(r-1, c-1)}}
$$

Where:
- \( \chi^2 \) = Chi-squared statistic
- \( n \) = Total number of observations
- \( r \), \( c \) = Number of rows and columns in the contingency table, respectively.

The threshold for interpreting the effect size is set at \( V \geq 0.3 \).

If there are zero counts in the contingency table, we apply the **Fisher's Exact Test**, which is suitable for small sample sizes and contingency tables with low expected frequencies. Fisher's Exact Test computes the exact probability of observing the given contingency table under the null hypothesis of independence. The test uses the hypergeometric distribution:

$$
P = \frac{{\binom{a+b}{a} \cdot \binom{c+d}{c}}}{{\binom{n}{a+c}}}
$$

Where:
- \( a, b, c, d \) = Counts in the contingency table cells.
- \( n \) = Total number of observations.

Here:
- \( \binom{x}{y} \) = Binomial coefficient, calculated as \( \binom{x}{y} = \frac{x!}{y!(x-y)!} \).

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

cat_variables = c("gender", "hypertension", "heart_disease", "ever_married", "Residence_type", "work_typechildren", "work_typeGovt_job", "work_typeSelf.employed", "smoking_statusformerly.smoked", "smoking_statussmokes")

for (index1 in 1:(length(cat_variables) - 1)) {
  var1 <- cat_variables[index1]
  
  for (index2 in (index1 + 1):length(cat_variables)) {
    var2 <- cat_variables[index2]
    
    contingency_table <- table(encoded_stroke_tb[[var1]], encoded_stroke_tb[[var2]])
    
    # Run Fisher's Exact Test if there are zero counts, otherwise Chi-square Test
    if (any(contingency_table == 0)) {
      chi_square_result <- fisher.test(contingency_table, simulate.p.value = TRUE)
    } else {
      chi_square_result <- chisq.test(contingency_table)
    }
    
    cramers_v <- assocstats(contingency_table)$cramer
    
    if (cramers_v > 0.3 && chi_square_result$p.value < 0.05) {
      cat("Relationship between", var1, "and", var2, "\n")
      cat("P-value:", chi_square_result$p.value, "\n")
      cat("Cramér's V:", cramers_v, "\n")
      print(contingency_table)
      cat("\n")
    }
  }
}
```

The only relationship, which seems to be problematic in terms of
multicollinearity seems to be between `ever_married` and
`work_typechildren`. This makes a lot of sense, as children are not yet
married and therefore a strong correlation will consist between
`ever_married` (0) and `work_typechildren` (1). For the predictive
models, we will therefore exclude `work_typechildren` because evidence
and our EDA suggests that stroke risk is correlated with age, making
this variable a weak predictor.

```{r}
#| code-fold: true
#| code-summary: "Click to show code"
# Drop work_typechildren
encoded_stroke_tb <- encoded_stroke_tb %>% 
  select(-work_typechildren)

str(encoded_stroke_tb)
```

As all other effect sizes appear to be smaller than 0.3, we can safely
conclude that all other categorical features are sufficiently
independent to use as predictors. In conclusion, multicollinearity does
not appear to be present among the variables, and we can therefore
proceed. The correlation matrix in the previous chapter indicated low
correlations between features as well. We therefore refrain from
conducting a multivariate analysis, as the results from this section
seem robust enough to also conclude no multicollinearity among multiple
features.

### Balancing & Stratified Sampling

The dataset revealed that most individuals do not have strokes, creating therefore 
a strong class imbalance. This imbalance can negatively impact model
performance by biasing it towards the majority class, resulting in poor
predictive accuracy for minority cases like strokes. Without applying
upsampling, the model might overly favor "no stroke" predictions,
yielding high overall accuracy but failing to identify actual stroke
cases effectively (low recall). By upsampling the minority class, we
ensure the model learns from stroke cases as well, improving its ability
to detect strokes and offering a more balanced and meaningful predictive
performance.

Let's start by creating the two stratified datasets, where we take the
age of 60 to split our sample in two groups. As seen in our EDA,
individuals aged over 60 have a significantly higher risk of stroke.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show the code"
# Creation of Low-Risk Age Group
low_risk_age_tb <- encoded_stroke_tb %>%
  filter(age < 60)
# Creation of High-Risk Age Group
high_risk_age_tb <- encoded_stroke_tb %>%
  filter(age >= 60)
```

Let's look at stroke occurrences in our three datasets:

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show the code"

# Count the number of people who had a stroke and calculate the proportion
encoded_stroke_tb %>%
  group_by(stroke) %>%
  summarize(count = n()) %>%
  mutate(proportion = round(count / sum(count), 2))

low_risk_age_tb %>%
  group_by(stroke) %>%
  summarize(count = n()) %>%
  mutate(proportion = round(count / sum(count), 2))

high_risk_age_tb %>%
  group_by(stroke) %>%
  summarize(count = n()) %>%
  mutate(proportion = round(count / sum(count), 2))
```

To address this issue, we use dataset balancing techniques to adjust the
proportions of stroke and no-stroke cases. Given that the dataset
contains only 5,110 individuals, we chose upsampling the minority class
to retain all available data, which would otherwise be reduced through
downsampling. By upsampling, we enhance the representation of stroke
cases, helping the model learn more effectively and improve both
prediction accuracy and recall, particularly for the minority class
(here when strokes occur).

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show the code"

# Use upSample to balance the dataset
balanced_stroke_tb <- upSample(
  x = encoded_stroke_tb[, names(encoded_stroke_tb) != "stroke"], 
  y = encoded_stroke_tb$stroke, 
  yname = "stroke"
)

high_risk_age_data <- upSample(
  x = high_risk_age_tb[, names(high_risk_age_tb) != "stroke"], 
  y = high_risk_age_tb$stroke, 
  yname = "stroke"
)

# Check the distribution to confirm balancing
table(balanced_stroke_tb$stroke)
```

### Scaling

Finally, as classification algorithms are often sensitive to feature
scales, we standardize all continuous features in the dataset to prevent
perturbations and improve model performance. Standardization rescales
features to have a mean of 0 and a standard deviation of 1. The
mathematical formula for standardization is as follows:

$$
z = \frac{x - \mu}{\sigma}
$$

Where: 
- z is the standardized value, 
- x is the original feature
value, 
- mu is the mean of the feature, 
- sigma is the standard deviation of the feature.

The scaling will be done after the train-test split to avoid data leakage. 

# Modeling

We divide our analysis into four different datasets and predictive
models: the original dataset, the balanced original dataset, the
low-risk age group, and the high-risk age group. We will train a
logistic regression model for each dataset, before evaluating them on
the test set. We will then compare the performance of the models and
discuss potential strengths and weaknesses.

## Metrics and Terms

First of all we introduce the metrics we will be using to evaluate our
models.

-   Confusion matrix The confusion matrix compares the actual values of
    the target variable with the predicted values and serves as a basis
    for our evaluation. It is a 2x2 matrix that contains the following
    values: True Positives (predicting correctly a stroke), True
    Negatives (predicting correctly no stroke occurrence), False
    Positives (predicting a stroke when there is none) and False
    Negatives (predicting no stroke when there is one). The confusion
    matrix is used to calculate the following metrics:

-   Precision

Precision is the ratio of correctly predicted positive observations to
the total predicted positive observations. The formula for precision is
given by:

$$
Precision = \frac{TP}{TP + FP}
$$

-   Recall Recall is the ratio of correctly predicted positive
    observations to the all observations in actual class. In our case
    and general in healthcare research, recall should be prioritized as
    undetected strokes are generally more fatal than false alarms. The
    formula for recall is given by:

$$
Recall = \frac{TP}{TP + FN}
$$

-   F1 Score The F1 score is the harmonic mean of Precision and Recall
    and often used in unbalanced dataset like ours as it considers false
    positives and false negatives. The formula for F1 score ranges from
    0 to 1 and is given by:

$$
F1 = 2×\frac{Precision × Recall}{Precision + Recall}
$$

-   Specificity Specificity is the ratio of correctly predicted negative
    observations to the total actual negative observations. The formula
    for specificity is given by:

$$
Specificity = \frac{TN}{TN + FP}
$$

-   Accuracy Accuracy is the ratio of correctly predicted observations
    to the total observations. The formula for accuracy is given by:

$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$


- Precision/Recall AUC and PR-curve


-   ROC Curve and AUC The *receiver operating characteristic* curve is a
    graphical representation of the true positive rate (recall) against
    the false positive rate (specificity). Concretely, it shows the
    tradeoff between sensitivity and specificity. The diagonal line
    represents the random classifier line. A good model stays as far
    away from the line as possible and tends to fit to the upper left
    corner. The area under the curve (AUC) is a measure of how well the
    model can distinguish between classes and serves primarly to compare
    models between eachother. An AUC of 1 indicates a perfect model,
    while an AUC of 0.5 indicates a model that performs no better than
    random chance.

-   Odds ratio The odds ratio is the exponentiated coefficient of a
    logistic regression model. It represents the change in odds of the
    target variable for a one-unit change in the predictor variable. An
    odds ratio greater than 1 indicates a positive relationship between
    the predictor and the target variable, while an odds ratio less than
    1 indicates a negative relationship. It will help us to understand
    the importance and direction of each feature in predicting the
    target variable.

## Naive Model

We divide our analysis into four different datasets and predictive
models: the original dataset, the balanced original dataset, the
low-risk age group, and the high-risk age group. We will train a
logistic regression model for each dataset, before evaluating them on
the test set. We will then compare the performance of the models and
discuss potential strengths and weaknesses.

### Train-test split

In order to evaluate the performance of our model, we split the original
dataset into a training and a test set. We use 80% of the data for
training and 20% for testing.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Load the dataset
unbalanced_original_data <- encoded_stroke_tb

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
unbalanced_original_data[cat_variables] <- lapply(unbalanced_original_data[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
unbalanced_original_data$stroke <- factor(
  unbalanced_original_data$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)

# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(unbalanced_original_data$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_unbalanced <- unbalanced_original_data[trainIndex, ]
test_unbalanced <- unbalanced_original_data[-trainIndex, ]

# Scaling
continuous_vars <- c("age", "avg_glucose_level", "bmi")

# Compute scaling parameters from training data only
train_means <- sapply(train_unbalanced[continuous_vars], mean, na.rm = TRUE)
train_sds   <- sapply(train_unbalanced[continuous_vars], sd, na.rm = TRUE)

# Define a function to standardize based on pre-computed parameters
standardize_with_params <- function(x, mean_val, sd_val) {
  (x - mean_val) / sd_val
}

# Apply scaling to the training set using training parameters
scaled_train_unbalanced <- train_unbalanced
for (var in continuous_vars) {
  scaled_train_unbalanced[[var]] <- standardize_with_params(scaled_train_unbalanced[[var]], 
                                               train_means[var], 
                                               train_sds[var])
}

# Apply the same scaling to the test set using *training* parameters
for (var in continuous_vars) {
  test_unbalanced[[var]] <- standardize_with_params(test_unbalanced[[var]], 
                                              train_means[var], 
                                              train_sds[var])
}
```

### Model

With the `caret` package, we specify our first logistic model by
specifying (method= "glm") and (family = "binomial") to train a logistic
regression model.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      # No cross-validation
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_1 <- train(
  stroke ~ ., 
  data = scaled_train_unbalanced,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```
### Evaluation and Discussion

We start by evaluating the model on the test.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_1, newdata = test_unbalanced, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_unbalanced$stroke <- factor(test_unbalanced$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_unbalanced$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_unbalanced$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_unbalanced$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Compute scaling parameters from the training data only
train_means <- sapply(train_unbalanced[continuous_vars], mean, na.rm = TRUE)
train_sds   <- sapply(train_unbalanced[continuous_vars], sd, na.rm = TRUE)

# Define the renaming function for predictors (unchanged)
rename_variables <- function(variable) {
  case_when(
    variable == "age" ~ "Age",
    variable == "work_typeSelf.employed1" ~ "Work Type: Self-Employed",
    variable == "hypertension1" ~ "Hypertension",
    variable == "smoking_statussmokes1" ~ "Smoking Status: Smokes",
    variable == "work_typeGovt_job1" ~ "Work Type: Government Job",
    variable == "heart_disease1" ~ "Heart Disease",
    variable == "avg_glucose_level" ~ "Average Glucose Level",
    variable == "ever_married1" ~ "Ever Married",
    variable == "smoking_statusformerly.smoked1" ~ "Smoking Status: Formerly Smoked",
    variable == "gender1" ~ "Gender: Female",
    variable == "Residence_type1" ~ "Residence Type: Urban",
    variable == "bmi" ~ "BMI",
    TRUE ~ variable
  )
}

# Updated function to generate the importance table
generate_importance_table <- function(model, model_name = "Logistic Model", continuous_vars, train_sds) {
  # Extract coefficients
  coefficients <- summary(model)$coefficients
  
  # Convert to a data frame and adjust continuous estimates to the original scale
  variable_importance <- as.data.frame(coefficients) %>%
    rownames_to_column("OriginalPredictor") %>%
    mutate(
      # For continuous predictors, divide the coefficient by the training set SD
      AdjustedEstimate = if_else(
        OriginalPredictor %in% continuous_vars,
        Estimate / train_sds[OriginalPredictor],
        Estimate
      )
    ) %>%
    mutate(
      Predictor = rename_variables(OriginalPredictor),
      `Odds Ratio` = exp(AdjustedEstimate)
    ) %>%
    select(Predictor, AdjustedEstimate, `Odds Ratio`, `Pr(>|z|)`) %>%
    rename(
      Coefficient = AdjustedEstimate,
      `P-value` = `Pr(>|z|)`
    ) %>%
    filter(Predictor != "(Intercept)") %>%
    arrange(desc(`Odds Ratio`))
  
  # Display the table with a dynamic caption
  kable(variable_importance, caption = paste(model_name, "- Predictor Effect with Odds Ratios")) %>%
    kable_styling(full_width = FALSE, position = "center")
}

# Example usage (assuming 'logistic_model_1' is your trained model, 
generate_importance_table(logistic_model_1, "Logistic Model 1", continuous_vars, train_sds)

```

## Balanced Baseline Model

We repeat the same process for our balanced baseline model.

### Train-test split

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"


# Load the dataset
balanced_original_data <- balanced_stroke_tb

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
balanced_original_data[cat_variables] <- lapply(balanced_original_data[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
balanced_original_data$stroke <- factor(
  balanced_original_data$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)

# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(balanced_original_data$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_balanced <- balanced_original_data[trainIndex, ]
test_balanced <- balanced_original_data[-trainIndex, ]

# Scaling
continuous_vars <- c("age", "avg_glucose_level", "bmi")

# Compute scaling parameters from training data only
train_means <- sapply(train_balanced[continuous_vars], mean, na.rm = TRUE)
train_sds   <- sapply(train_balanced[continuous_vars], sd, na.rm = TRUE)

# Define a function to standardize based on pre-computed parameters
standardize_with_params <- function(x, mean_val, sd_val) {
  (x - mean_val) / sd_val
}

# Apply scaling to the training set using training parameters
scaled_train_balanced <- train_balanced
for (var in continuous_vars) {
  scaled_train_balanced[[var]] <- standardize_with_params(scaled_train_balanced[[var]], 
                                               train_means[var], 
                                               train_sds[var])
}

# Apply the same scaling to the test set using *training* parameters
for (var in continuous_vars) {
  test_balanced[[var]] <- standardize_with_params(test_balanced[[var]], 
                                              train_means[var], 
                                              train_sds[var])
}
```

### Model

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      # No cross-validation
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_2 <- train(
  stroke ~ ., 
  data = scaled_train_balanced,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```
### Evaluation and Discussion

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_2, newdata = test_balanced, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_balanced$stroke <- factor(test_balanced$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_balanced$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}

# Check Confusion Matrix Input
print(table(test_predictions_class, test_balanced$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_balanced$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_balanced$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Identify the continuous variables that were scaled
continuous_vars <- c("age", "avg_glucose_level", "bmi")

# Compute scaling parameters from the training data only
train_means <- sapply(train_balanced[continuous_vars], mean, na.rm = TRUE)
train_sds   <- sapply(train_balanced[continuous_vars], sd, na.rm = TRUE)

generate_importance_table(logistic_model_2, "Logistic Model 2", continuous_vars, train_sds)
```

## Low-Risk Age Model

In this section , we build our first stratified model for the low-risk
age group. Here we first split the original dataset into test and
training set and perform then the previously mentionned preprocessing
steps for the training set.

### Train-test split

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
low_risk_age_tb[cat_variables] <- lapply(low_risk_age_tb[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
low_risk_age_tb$stroke <- factor(
  low_risk_age_tb$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)

# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(low_risk_age_tb$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_low_risk <- low_risk_age_tb[trainIndex, ]
test_low_risk <- low_risk_age_tb[-trainIndex, ]

# Balance the training set
balanced_train_low_risk <- upSample(
  x = train_low_risk[, names(low_risk_age_tb) != "stroke"], 
  y = train_low_risk$stroke, 
  yname = "stroke"
)

# Scaling
continuous_vars <- c("age", "avg_glucose_level", "bmi")

# Compute scaling parameters from training data only
train_means <- sapply(train_low_risk[continuous_vars], mean, na.rm = TRUE)
train_sds   <- sapply(train_low_risk[continuous_vars], sd, na.rm = TRUE)

# Define a function to standardize based on pre-computed parameters
standardize_with_params <- function(x, mean_val, sd_val) {
  (x - mean_val) / sd_val
}

# Apply scaling to the training set using training parameters
scaled_train_low_risk <- balanced_train_low_risk
for (var in continuous_vars) {
  scaled_train_low_risk[[var]] <- standardize_with_params(scaled_train_low_risk[[var]], 
                                               train_means[var], 
                                               train_sds[var])
}

# Apply the same scaling to the test set using *training* parameters
for (var in continuous_vars) {
  test_low_risk[[var]] <- standardize_with_params(test_low_risk[[var]], 
                                              train_means[var], 
                                              train_sds[var])
}
```

### Model

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_3 <- train(
  stroke ~ ., 
  data = scaled_train_low_risk,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```
### Evaluation and Discussion

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_3, newdata = test_low_risk, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_low_risk$stroke <- factor(test_low_risk$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_low_risk$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}

# Check Confusion Matrix Input
print(table(test_predictions_class, test_low_risk$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_low_risk$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

probably due to several factors: - different stroke distribution in the
training set - overfitting to the training set structure - the small
size of the test set

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_low_risk$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Identify the continuous variables that were scaled
continuous_vars <- c("age", "avg_glucose_level", "bmi")

# Compute scaling parameters from the training data only
train_means <- sapply(train_low_risk[continuous_vars], mean, na.rm = TRUE)
train_sds   <- sapply(train_low_risk[continuous_vars], sd, na.rm = TRUE)

generate_importance_table(logistic_model_3, "Logistic Model 3", continuous_vars, train_sds)
```

## High-Risk Age Model

Our last model comprises all individuals aged 60 and above. As the variance in age is pretty low and the strata to a certain scale incorporates the information of `age` , we remove the feature. 

### Train-test split

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Load the dataset
high_risk_age_data <- high_risk_age_data %>% 
  select(-c("age")) 

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
high_risk_age_data[cat_variables] <- lapply(high_risk_age_data[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
high_risk_age_data$stroke <- factor(
  high_risk_age_data$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)


# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(high_risk_age_data$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_high_risk <- high_risk_age_data[trainIndex, ]
test_high_risk <- high_risk_age_data[-trainIndex, ]


# Scaling
continuous_vars <- c("avg_glucose_level", "bmi")

# Compute scaling parameters from training data only
train_means <- sapply(train_high_risk[continuous_vars], mean, na.rm = TRUE)
train_sds   <- sapply(train_high_risk[continuous_vars], sd, na.rm = TRUE)

# Define a function to standardize based on pre-computed parameters
standardize_with_params <- function(x, mean_val, sd_val) {
  (x - mean_val) / sd_val
}

# Apply scaling to the training set using training parameters
scaled_train_high_risk <- train_high_risk
for (var in continuous_vars) {
  scaled_train_high_risk[[var]] <- standardize_with_params(scaled_train_high_risk[[var]], 
                                               train_means[var], 
                                               train_sds[var])
}

# Apply the same scaling to the test set using *training* parameters
for (var in continuous_vars) {
  test_high_risk[[var]] <- standardize_with_params(test_high_risk[[var]], 
                                              train_means[var], 
                                              train_sds[var])
}
```


### Model

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_4 <- train(
  stroke ~ ., 
  data = scaled_train_high_risk,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```
### Evaluation and Discussion

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_4, newdata = test_high_risk, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_high_risk$stroke <- factor(test_high_risk$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_high_risk$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}
  
# Check Confusion Matrix Input
print(table(test_predictions_class, test_high_risk$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_high_risk$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
    Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_high_risk$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Identify the continuous variables that were scaled
continuous_vars <- c("avg_glucose_level", "bmi")

# Compute scaling parameters from the training data only
train_means <- sapply(train_high_risk[continuous_vars], mean, na.rm = TRUE)
train_sds   <- sapply(train_high_risk[continuous_vars], sd, na.rm = TRUE)

generate_importance_table(logistic_model_4, "Logistic Model 4", continuous_vars, train_sds)
```


