---
title: "_data_cleaning"
format: html
---

# Data Cleaning & Wrangeling

```{r, warning=FALSE}
library(naniar)
library(tidyverse)
library(caret)
library(plotly)
library(knitr)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
stroke_tb <- read.csv("../../data/datasets/stroke_dataset.csv") 

kable(stroke_tb)

str(stroke_tb)
```
```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
colSums(stroke_tb == "Unknown" | stroke_tb == "N/A" | stroke_tb == "")
```
```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
sum(duplicated(stroke_tb))
```
The dataset seems to be fairly clean. Only the features `bmi` and `smoking_status` have missing values and no duplicated rows are present. We will adress them in the next step.

Our dataset contains several categorical variables. Since a machine learning model requires numerical inputs, we distinguish from the beginning categorical and numerical variables.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# identify categorical and numerical variables 
categorical_variables <- names(stroke_tb)[sapply(stroke_tb, function(x) is.character(x) || is.integer(x))]
numerical_variables <- names(stroke_tb)[sapply(stroke_tb, is.double)]

# print
cat(sprintf("Categorical features: %s\nNumerical features: %s",
            paste(categorical_variables, collapse = ", "),
            paste(numerical_variables, collapse = ", ")))
```
Here, we observe that some variables don't get recognized correctly. `bmi` is a numerical feature and `hypertension` and `heart_disease` ought to be categorical. We will address these issues in the next steps.

## Missing Values
Secondly, we address missing values in the features `bmi` and `smoking_status` by replacing them with `N/A`. Additionally, we ensure that `bmi` is recognized as a numerical variable and update the lists of categorical and numerical columns, as `bmi` is now a double. The `id` column is also removed, as it is not relevant for our analysis.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Clean the dataset
cleaned_stroke_tb <- replace_with_na(
  data = stroke_tb,
  replace = list(
    bmi = c("N/A", ""),  
    smoking_status = c("Unknown", "") 
  )
) %>%
  mutate(
    bmi = as.numeric(bmi)  # Convert bmi to numeric after handling "N/A"
  ) %>%
  select(-id)  # Remove the 'id' column

kable(cleaned_stroke_tb)

# Identify categorical variables (character or factor)
categorical_variables <- names(cleaned_stroke_tb)[sapply(cleaned_stroke_tb, function(x) is.character(x) || is.integer(x))]
numerical_variables <- names(cleaned_stroke_tb)[sapply(cleaned_stroke_tb, is.double)]

# Store the vectors
categorical_vars <- categorical_variables
numerical_vars <- numerical_variables

# Print the vectors to check
cat("Categorical Variables:\n", paste(categorical_vars, collapse = ", "), "\n")
cat("Numerical Variables:\n", paste(numerical_vars, collapse = ", "), "\n")

# Save 
write.csv(cleaned_stroke_tb, "../../data/datasets/cleaned_stroke_tb.csv", row.names = FALSE)

```
Then, to maintain the number of observations in our dataset, we replace the `N/A` values in `bmi` with the median and in `smoking_status` with the modal value.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Calculate the median for bmi and the mode for smoking_status
bmi_median <- median(cleaned_stroke_tb$bmi, na.rm = TRUE)
smoking_status_mode <- names(which.max(table(cleaned_stroke_tb$smoking_status, useNA = "no")))

# Create preprocessed data by replacing NA values
preprocessed_stroke_tb <- cleaned_stroke_tb %>%
  mutate(
    bmi = ifelse(is.na(bmi), bmi_median, bmi),
    smoking_status = ifelse(is.na(smoking_status), smoking_status_mode, smoking_status)
  )

kable(preprocessed_stroke_tb)

write.csv(preprocessed_stroke_tb, "../../data/datasets/preprocessed_stroke_tb.csv", row.names = FALSE)
```

## Outlier Analysis

We notice that there is only three non-binary quantitative variables: `age`, `avg_glucose_level` and `bmi`. To visualize outliers in each of these variables, we use boxplots. The points outside the range defined by the whiskers were identified as outliers.
```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Create interactive boxplots for each variable
variables <- c("age", "avg_glucose_level", "bmi")

for (var in variables) {
  p <- plot_ly(data, y = ~get(var), type = "box", name = var,
               boxpoints = "outliers", marker = list(color = 'red')) %>%
    layout(title = paste("Boxplot of", var), yaxis = list(title = var))
  print(p)
}

```


## Dataset Balancing

The dataset reveals that most individuals do not have strokes, creating a strong class imbalance. This imbalance can negatively impact model performance by biasing it towards the majority class, resulting in poor predictive accuracy for minority cases like strokes. Without applying upsampling, the model might overly favor "no stroke" predictions, yielding high overall accuracy but failing to identify actual stroke cases effectively (low recall). By upsampling the minority class, we ensure the model learns from stroke cases as well, improving its ability to detect strokes and offering a more balanced and meaningful predictive performance.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show the code"
# Plot the distribution of stroke cases
ggplot(outlier_stroke_tb, aes(x = factor(stroke))) + 
  geom_bar() + 
  labs(x = '0 = No Stroke, 1 = Stroke', y = 'Count', title = 'Distribution of Strokes')

# Count the number of people who had a stroke and calculate the proportion
encoded_stroke_tb %>%
  group_by(stroke) %>%
  summarize(count = n()) %>%
  mutate(proportion = round(count / sum(count), 2))
```
To address this, we used dataset balancing techniques to adjust the proportions of stroke and no-stroke cases. Given that the dataset contains only 5,110 individuals, we chose upsampling the minority class to retain all available data, which would otherwise be reduced through downsampling. By upsampling, we enhance the representation of stroke cases, helping the model learn more effectively and improve both prediction accuracy and recall, particularly for the minority class.

```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show the code"

# Use upSample to balance the dataset
balanced_stroke_tb <- upSample(
  x = outlier_stroke_tb[, names(outlier_stroke_tb) != "stroke"], # All columns except 'stroke'
  y = outlier_stroke_tb$stroke, # Target variable for upsampling
  yname = "stroke" # Column name for the target variable in the upsampled dataset
)


# Check the structure to confirm changes
str(balanced_stroke_tb)

# Check the distribution to confirm balancing
table(balanced_stroke_tb$stroke)

# Save the dataset
write.csv(balanced_stroke_tb, "../../data/datasets/balanced_stroke_tb.csv", row.names = FALSE)

```

## Feature scaling

Finally, as classification algorithms are often sensitive to scales and to prevent perturbations, we standardize all continuous features in the dataset.
```{r, warning=FALSE}
#|code-fold: true
#|code-summary: "Click to show code"

scaled_stroke_tb <- balanced_stroke_tb

scaled_unbalanced_stroke_tb <- outlier_stroke_tb

#Scale only the double columns

scaled_stroke_tb[sapply(scaled_stroke_tb, is.double)] <- lapply(scaled_stroke_tb[sapply(scaled_stroke_tb, is.double)], scale)

scaled_stroke_tb <- as_tibble(scaled_stroke_tb)

head(scaled_stroke_tb)


scaled_unbalanced_stroke_tb[sapply(scaled_unbalanced_stroke_tb, is.double)] <- lapply(scaled_unbalanced_stroke_tb[sapply(scaled_unbalanced_stroke_tb, is.double)], scale)

scaled_unbalanced_stroke_tb <- as_tibble(scaled_unbalanced_stroke_tb)

# save
write.csv(scaled_stroke_tb, "../../data/datasets/scaled_stroke_tb.csv", row.names = FALSE)
write.csv(scaled_unbalanced_stroke_tb, "../../data/datasets/scaled_unbalanced_stroke_tb.csv", row.names = FALSE)
```