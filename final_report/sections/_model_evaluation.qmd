---
title: "_model_evaluation"
format: html
---
```{r}
library(caret)   
library(pROC)    
library(ggplot2)
library(tidyverse)
library(kableExtra)
```

# Model Evaluation

In this final section, we analyze our models performance on unseen data and compare how they perform. First of all we introduce the metrics we will be using to evaluate our models. 

- Confusion matrix
The confusion matrix compares the actual values of the target variable with the predicted values and serves as a basis for our evaluation. It is a 2x2 matrix that contains the following values: True Positives (predicting correctly a stroke), True Negatives (predicting correctly no stroke occurrence), False Positives (predicting a stroke when there is none) and False Negatives (predicting no stroke when there is one). The confusion matrix is used to calculate the following metrics:

- Precision
Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The formula for precision is given by:

$$
Precision = \frac{TP}{TP + FP}
$$
- Recall
Recall is the ratio of correctly predicted positive observations to the all observations in actual class. In our case and general in healthcare research, recall should be prioritized as undetected strokes are generally more fatal than false alarms. The formula for recall is given by: 

$$
Recall = \frac{TP}{TP + FN}
$$
- F1 Score
The F1 score is the harmonic mean of Precision and Recall and often used in unbalanced dataset like ours as it considers false positives and false negatives. The formula for F1 score ranges from 0 to 1 and is given by:
$$
F1 = 2×\frac{Precision × Recall}{Precision + Recall}
$$
- Specificity
Specificity is the ratio of correctly predicted negative observations to the total actual negative observations. The formula for specificity is given by:
$$
Specificity = \frac{TN}{TN + FP}
$$
- Accuracy
Accuracy is the ratio of correctly predicted observations to the total observations. The formula for accuracy is given by:
$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$
- ROC Curve and AUC
The *receiver operating characteristic* curve is a graphical representation of the true positive rate (recall) against the false positive rate (specificity). It shows the tradeoff between sensitivity and specificity. The area under the curve (AUC) is a measure of how well the model can distinguish between classes. An AUC of 1 indicates a perfect model, while an AUC of 0.5 indicates a model that performs no better than random chance.

- Odds ratio
The odds ratio is the exponentiated coefficient of a logistic regression model. It represents the change in odds of the target variable for a one-unit change in the predictor variable. An odds ratio greater than 1 indicates a positive relationship between the predictor and the target variable, while an odds ratio less than 1 indicates a negative relationship.

## Baseline Model
We start with our baseline model, which is a logistic regression model trained on the unbalanced dataset. We load the model and the test set, make predictions, and evaluate the model's performance using the metrics mentioned above.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

logistic_model_1 <- readRDS("../../data/models/logistic_model_1.rds")
test_unbalanced <- read.csv("../../data/datasets/test_sets/test_unbalanced.csv")
str(test_unbalanced)
```
```{r}
## Correct factor levels for binary variables
binary_vars <- c(
  "gender", "hypertension", "heart_disease", "ever_married",
  "Residence_type", "smoking_statusformerly_smoked", "smoking_statussmokes",
  "work_typechildren", "work_typeGovt_job", "work_typeSelf_employed", "stroke"
)

test_unbalanced[binary_vars] <- lapply(test_unbalanced[binary_vars], function(col) {
  # Drop any existing factor levels and redefine levels explicitly
  factor(as.character(col), levels = c("0", "1"), labels = c("0", "1"))
})

# Ensure the target variable (stroke) is labeled as "No" and "Yes"
test_unbalanced$stroke <- factor(as.character(test_unbalanced$stroke), levels = c("No", "Yes"))

# Verify structure after reformatting
str(test_unbalanced)

```


Let's start with the 
```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities and classes on the test set
test_predictions_prob <- predict(logistic_model_1, newdata = test_unbalanced, type = "prob")
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Generate Confusion Matrix Table
conf_matrix_test <- confusionMatrix(test_predictions_class, test_unbalanced$stroke, positive = "Yes")
conf_matrix_values <- as.data.frame.matrix(conf_matrix_test$table)

# Manually Create a Neatly Labeled Confusion Matrix
conf_matrix_table <- data.frame(
  Predicted_Negative = c(conf_matrix_values[1, 1], conf_matrix_values[2, 1]),
  Predicted_Positive = c(conf_matrix_values[1, 2], conf_matrix_values[2, 2])
)
rownames(conf_matrix_table) <- c("Actual_Negative", "Actual_Positive")

# Rename Columns
colnames(conf_matrix_table) <- c("Predicted_Negative", "Predicted_Positive")

# Extract Metrics
precision <- conf_matrix_test$byClass["Precision"]
recall <- conf_matrix_test$byClass["Recall"]
f1_score <- conf_matrix_test$byClass["F1"]
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

classification_report_test <- data.frame(
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Display Confusion Matrix with Clear and Professional Layout
kable(conf_matrix_table, format = "html", caption = "Confusion Matrix") %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  add_header_above(c(" " = 1, "Predicted" = 2))

# Display Cleaned Metrics Table
kable(classification_report_test, format = "html", caption = "Classification Metrics") %>%
  kable_styling(full_width = FALSE, position = "center")
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_unbalanced$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```
While the model demonstrates strengths in specificity and overall accuracy, its complete failure to identify stroke cases (recall = 0.0) makes it unsuitable for practical use. Stroke prediction requires a strong emphasis on recall to avoid false negatives, as the consequences of missed diagnoses are severe. Future iterations of the model must address class imbalance, optimize for recall, and explore alternative algorithms. By implementing these changes, the model can better fulfill its intended purpose of supporting timely and accurate stroke diagnosis.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Step 1: Mapping function for comprehensive variable names
rename_variables <- function(variable) {
  case_when(
    variable == "age" ~ "Age",
    variable == "work_typechildrenX1" ~ "Work Type: Children",
    variable == "work_type_self_employedX1" ~ "Work Type: Self-Employed",
    variable == "hypertensionX1" ~ "Hypertension",
    variable == "smoking_statussmokesX1" ~ "Smoking Status: Smokes",
    variable == "work_type_govt_jobX1" ~ "Work Type: Government Job",
    variable == "heart_diseaseX0" ~ "Heart Disease: No",
    variable == "avg_glucose_level" ~ "Average Glucose Level",
    variable == "ever_marriedX0" ~ "Ever Married: No",
    variable == "smoking_statusformerly_smokedX0" ~ "Smoking Status: Never Smoked",
    variable == "genderX1" ~ "Gender: Female",
    variable == "residence_typeX0" ~ "Residence Type: Rural",
    TRUE ~ variable
  )
}

# Step 2: Define a function to generate the importance table for a specific model
generate_importance_table <- function(model, model_name = "Logistic Model") {
  # Extract coefficients
  coefficients <- summary(model)$coefficients
  
  # Create the variable importance table
  variable_importance <- as.data.frame(coefficients) %>%
    rownames_to_column("Predictor") %>%             # Convert row names to a column
    mutate(
      Predictor = rename_variables(Predictor),      # Rename variables
      `Odds Ratio` = exp(Estimate)                 # Calculate Odds Ratios
    ) %>%
    select(Predictor, Estimate, `Odds Ratio`, `Pr(>|z|)`) %>%  # Keep relevant columns
    rename(
      Coefficient = Estimate,                      # Rename Estimate to Coefficient
      `P-value` = `Pr(>|z|)`                       # Rename P-value column
    ) %>%
    filter(Predictor != "(Intercept)") %>%           # Exclude the intercept
    arrange(desc(`Odds Ratio`))                # Sort by absolute value of coefficients
  
  # Display the table with a dynamic caption
  kable(variable_importance, caption = paste(model_name, "- Predictor Importance with Odds Ratios")) %>%
    kable_styling(full_width = FALSE, position = "center")
}
generate_importance_table(logistic_model_1, "Logistic Model 1")
```
The table given above more or less aligns with the expectations we've had since our EDA. 
Strong predictors of stroke include age, hypertension, heart disease, and average glucose level. The odds ratios indicate the strength and direction of the relationship between each predictor and the likelihood of stroke. For example, a one-unit increase in age is associated with a 1.06 times increase in the odds of stroke, while a one-unit increase in average glucose level is associated with a 1.01 times increase in the odds of stroke. 

## Balanced Baseline Model
```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

logistic_model_2 <- readRDS("../../data/models/logistic_model_2.rds")
test_balanced <- read.csv("../../data/datasets/test_sets/test_balanced.csv")
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities and classes on the test set
test_predictions_prob <- predict(logistic_model_2, newdata = test_balanced, type = "prob")
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert predictions to factors to match the test set
test_balanced$stroke <- factor(test_balanced$stroke, levels = c("No", "Yes"))
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Generate Confusion Matrix Table
conf_matrix_test <- confusionMatrix(test_predictions_class, test_balanced$stroke, positive = "Yes")
conf_matrix_values <- as.data.frame.matrix(conf_matrix_test$table)

# Manually Create a Neatly Labeled Confusion Matrix
conf_matrix_table <- data.frame(
  Predicted_Negative = c(conf_matrix_values[1, 1], conf_matrix_values[2, 1]),
  Predicted_Positive = c(conf_matrix_values[1, 2], conf_matrix_values[2, 2])
)
rownames(conf_matrix_table) <- c("Actual_Negative", "Actual_Positive")

# Rename Columns
colnames(conf_matrix_table) <- c("Predicted_Negative", "Predicted_Positive")

# Extract Metrics
precision <- conf_matrix_test$byClass["Precision"]
recall <- conf_matrix_test$byClass["Recall"]
f1_score <- conf_matrix_test$byClass["F1"]
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

classification_report_test <- data.frame(
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Display Confusion Matrix with Clear and Professional Layout
kable(conf_matrix_table, format = "html", caption = "Confusion Matrix") %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  add_header_above(c(" " = 1, "Predicted" = 2))

# Display Cleaned Metrics Table
kable(classification_report_test, format = "html", caption = "Classification Metrics") %>%
  kable_styling(full_width = FALSE, position = "center")

# ROC Curve
roc_curve <- roc(test_balanced$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```
The balanced model shows significant improvement in recall compared to the unbalanced model, with a recall of 0.75. This indicates that the balanced model is better at identifying stroke cases, reducing the number of false negatives. The model's precision, specificity, and accuracy have also improved, making it more reliable for stroke prediction. The ROC curve shows a strong performance, with an AUC of 0.85, indicating that the model can effectively distinguish between stroke and non-stroke cases. Overall, the balanced model is a significant improvement over the unbalanced model and demonstrates the importance of addressing class imbalance in the dataset. Nevertheless, our balanced model still misses a significant number of stroke cases, which is a critical issue in healthcare applications.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

generate_importance_table(logistic_model_2, "Logistic Model 2")
```
In our balanced model, the odds ratio seem to be fairly similar to the unbalanced model. The most important predictors of stroke are age, work_type_children, hypertension and smoking. 

## Low-Risk Age Model
```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
logistic_model_3 <- readRDS("../../data/models/logistic_model_3.rds")
test_low_risk <- read.csv("../../data/datasets/test_sets/test_low_risk.csv")
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities and classes on the test set
test_predictions_prob <- predict(logistic_model_3, newdata = test_low_risk, type = "prob")
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

length(test_predictions_class)
length(test_low_risk$stroke)


# Convert predictions to factors to match the test set
test_low_risk$stroke <- factor(test_low_risk$stroke, levels = c("No", "Yes"))
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

table(test_predictions_class)
table(test_low_risk$stroke)


# Generate Confusion Matrix Table
conf_matrix_test <- confusionMatrix(test_predictions_class, test_low_risk$stroke, positive = "Yes")
conf_matrix_values <- as.data.frame.matrix(conf_matrix_test$table)

conf_matrix_test$table

# Calculate values manually
TP <- sum(test_predictions_class == "Yes" & test_low_risk$stroke == "Yes")
FP <- sum(test_predictions_class == "Yes" & test_low_risk$stroke == "No")
FN <- sum(test_predictions_class == "No" & test_low_risk$stroke == "Yes")
TN <- sum(test_predictions_class == "No" & test_low_risk$stroke == "No")

# Print values
cat("TP:", TP, "FP:", FP, "FN:", FN, "TN:", TN, "\n")



# Manually Create a Neatly Labeled Confusion Matrix
conf_matrix_table <- data.frame(
  Predicted_Negative = c(conf_matrix_values[1, 1], conf_matrix_values[2, 1]),
  Predicted_Positive = c(conf_matrix_values[1, 2], conf_matrix_values[2, 2])
)
rownames(conf_matrix_table) <- c("Actual_Negative", "Actual_Positive")

# Rename Columns
colnames(conf_matrix_table) <- c("Predicted_Negative", "Predicted_Positive")

# Extract Metrics
precision <- conf_matrix_test$byClass["Precision"]
recall <- conf_matrix_test$byClass["Recall"]
f1_score <- conf_matrix_test$byClass["F1"]
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

classification_report_test <- data.frame(
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Display Confusion Matrix with Clear and Professional Layout
kable(conf_matrix_table, format = "html", caption = "Confusion Matrix") %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  add_header_above(c(" " = 1, "Predicted" = 2))

# Display Cleaned Metrics Table
kable(classification_report_test, format = "html", caption = "Classification Metrics") %>%
  kable_styling(full_width = FALSE, position = "center")

# ROC Curve
roc_curve <- roc(test_low_risk$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```
The confusion matrix shows that this model is heavily skewed towards predicting "no stroke," which inflates its true negatives while drastically underperforming in identifying true positives (represented by a weak precision of 2.8%).  In a real-world healthcare setting, this would be unacceptable for stroke detection because the primary goal is to identify as many strokes as possible, even at the cost of a slightly higher false positive rate.
```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
generate_importance_table(logistic_model_3, "Logistic Model 3")
```

## High-Risk Age Model 
```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
logistic_model_4 <- readRDS("../../data/models/logistic_model_4.rds")
test_high_risk <- read.csv("../../data/datasets/test_sets/test_high_risk.csv")
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities and classes on the test set
test_predictions_prob <- predict(logistic_model_4, newdata = test_high_risk, type = "prob")
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert predictions to factors to match the test set
test_high_risk$stroke <- factor(test_high_risk$stroke, levels = c("No", "Yes"))
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Generate Confusion Matrix Table
conf_matrix_test <- confusionMatrix(test_predictions_class, test_high_risk$stroke, positive = "Yes")
conf_matrix_values <- as.data.frame.matrix(conf_matrix_test$table)

# Manually Create a Neatly Labeled Confusion Matrix
conf_matrix_table <- data.frame(
  Predicted_Negative = c(conf_matrix_values[1, 1], conf_matrix_values[2, 1]),
  Predicted_Positive = c(conf_matrix_values[1, 2], conf_matrix_values[2, 2])
)
rownames(conf_matrix_table) <- c("Actual_Negative", "Actual_Positive")

# Rename Columns
colnames(conf_matrix_table) <- c("Predicted_Negative", "Predicted_Positive")

# Extract Metrics
precision <- conf_matrix_test$byClass["Precision"]
recall <- conf_matrix_test$byClass["Recall"]
f1_score <- conf_matrix_test$byClass["F1"]
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

classification_report_test <- data.frame(
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Display Confusion Matrix with Clear and Professional Layout
kable(conf_matrix_table, format = "html", caption = "Confusion Matrix") %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  add_header_above(c(" " = 1, "Predicted" = 2))

# Display Cleaned Metrics Table
kable(classification_report_test, format = "html", caption = "Classification Metrics") %>%
  kable_styling(full_width = FALSE, position = "center")

# ROC Curve
roc_curve <- roc(test_high_risk$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
generate_importance_table(logistic_model_4, "Logistic Model 4")
```

