---
title: "Understanding Stroke: A Data-Driven Approach"
subtitle: "Data Science in Business Analytics"
author: "Bolor Battaiwan, Fabio Pfenniger, Muhammed Hussein"
date: "2024-12-10"
format: 
  revealjs:
    self-contained: false
    monofont: DejaVu Sans Mono
    theme: default
    transition: slide
    slide-number: true
editor: visual
---

```{r}
#| child: "_data_preprocessing.R"
```

```{r}
#| child: "_eda.R"
```

```{r}
#| child: "_predictive_model_final.R"
```


```{r, message= FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
#| label: setup
#| echo: false

# loading all the necessary packages
library(tidyverse)
library(shiny)
library(plotly)
library(vcd)
library(naniar)
library(fastDummies)
library(caret)
library(knitr)
```

# Stroke: A Global Health Emergency

## The Problem:

-   15M strokes/year (WHO): 5M deaths, 5M disabilities.
-   Stroke disrupts brain blood flow: caused by blockage or bleeding.

## Project Goals:

-   Predict stroke risk using data.
-   Analyze key factors like age, heart disease, & lifestyle.
-   Visualize insights for better understanding.

## Key Question: What factors best predict stroke, and how can we intervene?

# Dataset Overview

## Data Source

-   Origin: Found on Kaggle.com. Owned and last updated in 2020 by Federico Soriano Palacios.
-   Observations: 5110
-   Features: 11

## Key Variables

-   Target Variable: stroke (0 = No, 1 = Yes)
-   Demographics: gender, age, Residence_type
-   Health Metrics: hypertension, heart_disease, avg_glucose_level, bmi
-   Lifestyle: smoking_status, work_type, ever_married

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Original Dataset
stroke_tb <- read.csv("../data/datasets/stroke_dataset.csv")
kable(stroke_tb)

```

# Data Preprocessing

## Original dataset:

Dataset is fairly clean because - No duplicate rows - Missing values only in bmi and smoking_status

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
colSums(stroke_tb == "Unknown" | stroke_tb == "N/A" | stroke_tb == "")
```

## Processed dataset:

Variable Adjustments:

-Reclassified variables: bmi â†’ numerical; hypertension, heart_disease â†’ categorical. - Removed irrelevant column: id.

Handling Missing Values:

-   Replaced missing values: bmi â†’ median, smoking_status â†’ mode.

Outlier Analysis:

-   Visualized outliers in age, avg_glucose_level, and bmi using boxplots.
-   Outliers Identified:
-   avg_glucose_level: 12.25% of dataset (627 points).
-   bmi: 2.47% of dataset(126 points).
-   Retained outliers for now due to potential relevance as risk factors.

```{r}
#| code-fold: true
#| code-summary: "Click to show code"
library(ggplot2)
library(plotly)

preprocessed_stroke_tb -> read.csv("../data/datasets/preprocessed_stroke_tb.csv")

variables <- c("age", "avg_glucose_level", "bmi")

for (var in variables) {
  p <- ggplot(preprocessed_stroke_tb, aes_string(y = var)) +
    geom_boxplot(outlier.color = "red", outlier.size = 2) +
    labs(title = paste("Boxplot of", var), y = var, x = "") 
  
  # Convert to Plotly and apply partial_bundle to reduce size
  p_plotly <- ggplotly(p) %>% partial_bundle()
  print(p_plotly)
}
```

Categorical Variables:

-   Removed infrequent categories in gender (Other) and work_type (Never_worked).
-   Noted rare occurrences in heart_disease, hypertension, and stroke.

# Findings and Results

## Profile of Stroke Cases

From the initial EDA, we can observe that individuals are at more struck by stroke if

-   older than 60 years
-   avg_glucose_level above
-   have heart disease
-   are former or current smokers
-   have hypertension
-   are married
-   are self-employed or work in the private sector

# Descriptive statistics

Relevant Numerical Variables:

-   age: Weak positive relationship with stroke (r = 0.24).
-   avg_glucose_level: Very weak positive relationship with stroke (r = 0.13).

Relevant Categorical Variables: - heart_disease - smoking_status - hypertension - ever_married - work_type

Non-Relevant Variables

-   gender: No difference in stroke proportions between female and male categories.
-   residence_type: No difference in stroke proportions across urban and rural categories.
-   bmi: No relationship with stroke (r = 0.04).

## Stroke cases are more frequent at higher glucose levels and in the age range of 60 to 80 years.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Display combined histogram
combined_histograms
```

## There exist weak positive relationships between age and stroke, and avg_glucose_level and stroke. No relationship between bmi and stroke.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Display the correlation table with kable
kable(correlation_table, caption = "Correlation Coefficients with Stroke")
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# List of categorical variables
# Display the combined table with kable
knitr::kable(summary_table, caption = "Summary of Categorical Variables with Proportion", row.names = FALSE)
```

```{r}
#| code-fold: true
#| code-summary: "Click to show code"
# Display all plots
for (plot in plots) {
  print(plot)  # You can use print to display each plot one after the other
}
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Plot the correlation matrix as a heatmap using plotly
plot_ly(
  x = colnames(correlation_matrix),
  y = rownames(correlation_matrix),
  z = correlation_matrix,
  type = "heatmap",
  colorscale = "Viridis"  # Corrected typo
) %>%
  layout(
    title = "Correlation Matrix Heatmap",
    xaxis = list(title = "", tickangle = 45),
    yaxis = list(title = "")
  )
```

# Predictive Modeling

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Display the scaled dataset
kable(scaled_stroke_tb)
```

## Analysis Overview

We divide the analysis into four datasets for model training and evaluation: 1. Original Dataset 2. Balanced Original Dataset 3. Low-Risk Age Group 4. High-Risk Age Group

Modeling Approach â€¢ Train a logistic regression model for each dataset. â€¢ Evaluate model performance on a shared test set.

# Results: 1. Original Dataset

## Baseline Metrics

-   Accuracy: High at 95.08%, but misleading due to poor performance on the minority class (Yes).
-   Specificity: Nearly perfect at 99.97%, indicating strong prediction of the majority class (No).
-   Recall (Positive Class): Extremely low at 0.5%, identifying very few true positives.
-   F1-Score: Very low at 0.99%, reflecting poor balance between precision and recall.

## Interpretation

-   The baseline model is highly biased toward the majority class (Stroke = No).
-   Poor performance on the minority class (Stroke = Yes) makes it unsuitable for predicting strokes effectively.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)

```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
generate_importance_table(logistic_model_1, "Logistic Model 1")
```

# Results: 2. Balanced Original Dataset

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)
```

## Improved Metrics

-   Accuracy: 77.45% - Strikes a better balance between positive and negative classes.
-   Recall (Positive Class): 81.35% - Effectively identifies most true positives.
-   F1-Score: 78.29% - Indicates a good balance between precision and recall.
-   Specificity: 73.5% - Lower, suggesting some false positives.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

generate_importance_table(logistic_model_2, "Logistic Model 2")
```

# Results: 3. Low-Risk Age Group

## Objective: Build the first stratified model for the low-risk age group.

## Approach

-   Dataset Split: Divide the original dataset into training and test sets.
-   Preprocessing: Apply previously mentioned preprocessing steps to the training set.

## Next Steps

-   Train a logistic regression model on the low-risk age group training data.
-   Evaluate model performance on the test set.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)

```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

Probably due to several factors: - different stroke distribution in the training set - overfitting to the training set structure - the small size of the test set

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

generate_importance_table(logistic_model_3, "Logistic Model 3")
```

# Results: 4. High-Risk Age Group

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)

```

## Performance Metrics

-   Accuracy: 63.70% - Moderate performance overall.
-   Recall (Positive Class): 66.11% - Captures a fair portion of true positives.
-   Precision: 63.07% - About 63% of predicted positives are correct.
-   F1-Score: 64.56% - Reasonable balance between precision and recall.
-   Specificity: 61.30% - Low, leading to higher false-positive rates.

## Intetpretation

-   Strength: Moderate ability to capture true positives.
-   Weakness: Struggles to distinguish between classes, with low specificity and accuracy.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
generate_importance_table(logistic_model_4, "Logistic Model 4")
```

# Conclusion

Order four models Best Secondbest ...

# Grading criteria:

Criteria (Max Points 20): Description

Content (4): Clear and comprehensive explanation of the project, its goals, and outcomes.

Organization (4): Logical flow of information, from introduction to conclusion.

Teamwork (4): Demonstration of collaboration and presentation dynamics (individual projects automatically earn full points).

Visuals (4): Well-designed and relevant slides or visual aids that effectively convey information.

Presentation Mechanics (4): Clear speaking, good video/audio quality, and adherence to time limits.

# Requirements:

Introduction: Briefly introduce your project goals and motivation.

Research Questions: What were the key research questions?

Data: Provide a high-level overview of your data sources and preprocessing.

Findings and Results: Highlight your key findings, insights, or model results.

Conclusion: Summarize your main takeaways and future potential directions.

Length: 7 minutes or less. The time limit will be strictly enforced. Content:

Tell your story well! Focus on delivering clear, concise, and engaging insights from your project. ðŸš€

# Remarks for presentation slides:

Lets not show results in the presentation that are not relevant. Aim remains to predict stroke.
