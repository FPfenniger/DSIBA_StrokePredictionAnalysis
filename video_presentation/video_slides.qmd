---
title: "Understanding Stroke: A Data-Driven Approach"
format: revealjs
editor: visual
---

```{r, warning=FALSE}
library(naniar)
library(tidyverse)
library(caret)
library(plotly)
library(knitr)
```

```{r, warning=FALSE}
library(tidyverse)
library(plotly)
library(knitr)
library(caret)
library(vcd)
```

```{r}
library(caret)
library(tidyverse)
library(vcd)
library(knitr)
library(janitor)
library(caret)  
library(MLmetrics)  
library(pROC)
library(kableExtra)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
outlier_stroke_tb <- 
  read.csv("../../data/datasets/outlier_stroke_tb.csv")
balanced_stroke_tb <-
  read.csv("../../data/datasets/balanced_stroke_tb.csv")
preprocessed_stroke_tb <-
  read.csv("../../data/datasets/preprocessed_stroke_tb.csv")
```

## Stroke: A Global Health Emergency

The Problem:

```         
•   15M strokes/year (WHO): 5M deaths, 5M disabilities.
•   Stroke disrupts brain blood flow: caused by blockage or bleeding.
```

Project Goals:

```         
•   Predict stroke risk using data.
•   Analyze key factors like age, heart disease, & lifestyle.
•   Visualize insights for better understanding.
```

Key Question:

What factors best predict stroke, and how can we intervene?

## Dataset Overview

Data Source

```         
    •   Origin: Found on Kaggle.com. Owned and last updated in 2020 by Federico Soriano Palacios.
    •   Observations: 5110
    •   Features: 11
```

Key Variables

```         
    •   Target Variable: stroke (0 = No, 1 = Yes)
    •   Demographics: gender, age, Residence_type
    •   Health Metrics: hypertension, heart_disease, avg_glucose_level, bmi
    •   Lifestyle: smoking_status, work_type, ever_married
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Original Dataset
kable(stroke_tb)

```

## Data Preprocessing

Original dataset:

```         
Dataset is fairly clean
    •   No duplicate rows.
    •   Missing values in bmi and smoking_status.
```

Processed dataset:

```         
Variable Adjustments:
•   Reclassified variables: bmi → numerical; hypertension, heart_disease → categorical.
•   Removed irrelevant column: id.

Handling Missing Values:
•   Replaced missing values: bmi → median, smoking_status → mode.

Outlier Analysis:
•   Visualized outliers in age, avg_glucose_level, and bmi using boxplots.
•   Outliers Identified:
•   avg_glucose_level: 12.25% of dataset (627 points).
•   bmi: 2.47% of dataset(126 points).
•   Retained outliers for now due to potential relevance as risk factors.
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

variables <- c("age", "avg_glucose_level", "bmi")

# Loop through each variable and render the plots
for (var in variables) {
  p <- ggplot(preprocessed_stroke_tb, aes_string(y = var)) +
    geom_boxplot(outlier.color = "red", outlier.size = 2) +
    labs(title = paste("Boxplot of", var), y = var, x = "") 
  
  # Explicitly print the plot
  print(ggplotly(p))
}
```

Categorical Variables:

```         
•   Removed infrequent categories in gender (Other) and work_type (Never_worked).
•   Noted rare occurrences in heart_disease, hypertension, and stroke.
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
colSums(stroke_tb == "Unknown" | stroke_tb == "N/A" | stroke_tb == "")
```

## Findings and Results

### Profile of Stroke Cases

From the initial EDA, we can observe that individuals are at more struck by stroke if

```         
    • older than 60 years
    • avg_glucose_level above
    • have heart disease
    • are former or current smokers
    • have hypertension
    • are married
    • are self-employed or work in the private sector
```

### Descriptive statistics

Relevant Numerical Variables:

```         
    •   age: Weak positive relationship with stroke (r = 0.24).
    •   avg_glucose_level: Very weak positive relationship with stroke (r = 0.13).
```

Relevant Categorical Variables:

```         
    •   heart_disease
    •   smoking_status
    •   hypertension
    •   ever_married
    •   work_type
```

Non-Relevant Variables

```         
    •   gender: No difference in stroke proportions between female and male categories.
    •   residence_type: No difference in stroke proportions across urban and rural categories.
    •   bmi: No relationship with stroke (r = 0.04).
```

### Plot annotation: Stroke cases are more frequent at higher glucose levels and in the age range of 60 to 80 years.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
library(ggplot2)
library(plotly)

# Function to create the histogram as plotly object
create_histogram <- function(data, variable, stroke_variable, binwidth = 5, show_legend = FALSE) {
  # Extract the variable and stroke data
  original_data <- data[[variable]]
  stroke_data <- data[[stroke_variable]]
  
  # Calculate mean and standard deviation for reference lines
  mean_val <- mean(original_data, na.rm = TRUE)
  sd_val <- sd(original_data, na.rm = TRUE)
  
  # Combine data and stroke status into a data frame
  plot_data <- data.frame(data = original_data, stroke = factor(stroke_data, labels = c("No Stroke", "Stroke")))
  
  # Generate the ggplot object (temporary)
  p <- ggplot(plot_data, aes(x = data, fill = stroke)) +
    geom_histogram(binwidth = binwidth, color = "white", position = "stack") +
    labs(y = "Frequency", fill = "Stroke Status") +
    scale_fill_manual(values = c("No Stroke" = "#1f77b4", "Stroke" = "#ff7f0e")) +  # Set custom colors
    theme_minimal() +
    theme(axis.title.x = element_blank(), plot.title = element_text(hjust = 0.5))  # Center title
  
  # Convert to interactive Plotly plot
  p_plotly <- ggplotly(p) %>%
    layout(showlegend = show_legend)  # Set legend visibility
  
  return(p_plotly)
}

# Generate histograms for each variable separately without repeating legends
hist_age <- create_histogram(outlier_stroke_tb, "age", "stroke", binwidth = 5, show_legend = FALSE) %>%
  layout(title = "Distribution of Age", xaxis = list(title = "Age"), 
         yaxis = list(title = "Frequency"))

hist_glucose <- create_histogram(outlier_stroke_tb, "avg_glucose_level", "stroke", binwidth = 5, show_legend = FALSE) %>%
  layout(title = "Distribution of Average Glucose Level", xaxis = list(title = "Average Glucose Level"), 
         yaxis = list(title = "Frequency"))

hist_bmi <- create_histogram(outlier_stroke_tb, "bmi", "stroke", binwidth = 5, show_legend = FALSE) %>%
  layout(title = "Distribution of BMI", xaxis = list(title = "BMI"), 
         yaxis = list(title = "Frequency"))

# Combine all histograms into a single interactive plot with a main title
combined_histograms <- subplot(hist_age, hist_glucose, hist_bmi, nrows = 1, titleY = TRUE,
                               shareX = TRUE, shareY = TRUE) %>%
  layout(title = "Histograms of Numerical Variables by Stroke", showlegend = TRUE)  # Set main title with legend visible

# Display combined histogram
combined_histograms
```

### Plot annotation: There exist weak positive relationships between age and stroke, and avg_glucose_level and stroke. No relationship between bmi and stroke.

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
library(knitr)

# Calculate correlation coefficients for each variable with stroke
cor_age <- cor(as.numeric(outlier_stroke_tb$age), as.numeric(outlier_stroke_tb$stroke), method = "pearson")
cor_glucose <- cor(as.numeric(outlier_stroke_tb$avg_glucose_level), as.numeric(outlier_stroke_tb$stroke), method = "pearson")
cor_bmi <- cor(as.numeric(outlier_stroke_tb$bmi), as.numeric(outlier_stroke_tb$stroke), method = "pearson")

# Create a correlation table
correlation_table <- data.frame(
  Variable = c("Age", "Average Glucose Level", "BMI"),
  Correlation_with_Stroke = round(c(cor_age, cor_glucose, cor_bmi), 2)
)

# Display the correlation table with kable
kable(correlation_table, caption = "Correlation Coefficients with Stroke")
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# List of categorical variables
variables_to_summarize <- c("stroke", "gender", "ever_married", "work_type", "Residence_type", "smoking_status", "hypertension", "heart_disease")

# Convert binary integer variables to factors
outlier_stroke_tb <- outlier_stroke_tb %>%
  mutate(
    stroke = as.factor(stroke),
    hypertension = as.factor(hypertension),
    heart_disease = as.factor(heart_disease)
  )

# Calculate total count for proportion calculation
total_count <- nrow(outlier_stroke_tb)

# Create summary table 
summary_table <- bind_rows(
  lapply(variables_to_summarize, function(var) {
    outlier_stroke_tb %>%
      group_by(.data[[var]]) %>%
      summarize(
        Count = n(),
        Proportion = n() / total_count,
        .groups = "drop"
      ) %>%
      rename(Type = .data[[var]]) %>%
      mutate(
        Variable = var,
        Type = as.character(Type),
        Proportion = round(Proportion * 100, 2)  
      )
  })
) %>%
  select(Variable, Type, Count, Proportion) %>%  
  arrange(Variable, Type)                        

# Display the combined table with kable
knitr::kable(summary_table, caption = "Summary of Categorical Variables with Proportion", row.names = FALSE)
```

```{r}
#| code-fold: true
#| code-summary: "Click to show code"

library(ggplot2)
library(plotly)
library(dplyr)

# Define choices for the variable selection, excluding Gender and Residence Type
x_variable_choices <- c("Ever Married" = "ever_married",
                        "Work Type" = "work_type", 
                        "Smoking Status" = "smoking_status",
                        "Hypertension" = "hypertension", 
                        "Heart Disease" = "heart_disease")

# Create a filtered data frame based on all selected variables
filtered_data <- outlier_stroke_tb
# Remove "Other" for the gender variable
filtered_data <- filtered_data %>% filter(!(gender == "Other"))  # Simply check for 'Other' in gender

# Initialize an empty list to store plots
plots <- list()

# Loop through each categorical variable and create a bar chart
for (selected_variable in x_variable_choices) {
  
  # Get variable name
  variable_name <- names(x_variable_choices)[which(x_variable_choices == selected_variable)]
  
  if (variable_name %in% c("hypertension", "heart_disease")) {
    p <- ggplot(filtered_data, aes_string(x = selected_variable, fill = "as.factor(stroke)")) +
      geom_bar(position = "fill") +
      labs(title = paste("Distribution of Stroke by", variable_name),
           fill = "STROKE", y = "Ratio") +
      scale_x_continuous(breaks = c(0, 1), labels = c("No", "Yes")) +  # Custom x-axis for binary variables
      scale_fill_manual(values = c("0" = "#1f77b4", "1" = "#ff7f0e"))  # Set colors for no stroke and stroke
  } else {
    # For other categorical variables
    p <- ggplot(filtered_data, aes_string(x = selected_variable, fill = "as.factor(stroke)")) +
      geom_bar(position = "fill") +
      labs(title = paste("Distribution of Stroke by", variable_name),
           fill = "STROKE", y = "Ratio") +
      scale_fill_manual(values = c("0" = "#1f77b4", "1" = "#ff7f0e"))  # Set colors for no stroke and stroke
  }
  
  # Convert ggplot to an interactive plotly object and store it in the list
  plots[[variable_name]] <- ggplotly(p)
}
# Display all plots
for (plot in plots) {
  print(plot)  # You can use print to display each plot one after the other
}
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Select the numeric columns from your data
numeric_data <- outlier_stroke_tb[, c("age", "avg_glucose_level", "bmi")]

# Calculate the correlation matrix
correlation_matrix <- cor(numeric_data, use = "complete.obs")

# Convert the correlation matrix to a format that plotly can use (optional step)
heatmap_data <- as.data.frame(as.table(correlation_matrix))

# Plot the correlation matrix as a heatmap using plotly
plot_ly(
  x = colnames(correlation_matrix),
  y = rownames(correlation_matrix),
  z = correlation_matrix,
  type = "heatmap",
  colorscale = "Viridis"  # Corrected typo
) %>%
  layout(
    title = "Correlation Matrix Heatmap",
    xaxis = list(title = "", tickangle = 45),
    yaxis = list(title = "")
  )
```

### Predictive Modeling
Analysis Overview

We divide the analysis into four datasets for model training and evaluation:
	1.	Original Dataset
	2.	Balanced Original Dataset
	3.	Low-Risk Age Group
	4.	High-Risk Age Group

Modeling Approach
	•	Train a logistic regression model for each dataset.
	•	Evaluate model performance on a shared test set.

### Results: 1. Original Dataset

Baseline Metrics
```
	•	Accuracy: High at 95.08%, but misleading due to poor performance on the minority class (Yes).
	•	Specificity: Nearly perfect at 99.97%, indicating strong prediction of the majority class (No).
	•	Recall (Positive Class): Extremely low at 0.5%, identifying very few true positives.
	•	F1-Score: Very low at 0.99%, reflecting poor balance between precision and recall.
```

Interpretation
```
	•	The baseline model is highly biased toward the majority class (Stroke = No).
	•	Poor performance on the minority class (Stroke = Yes) makes it unsuitable for predicting strokes effectively.
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# Train-Test Split

# Load the dataset
unbalanced_original_data <- scaled_unbalanced_stroke_tb

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
unbalanced_original_data[cat_variables] <- lapply(unbalanced_original_data[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
unbalanced_original_data$stroke <- factor(
  unbalanced_original_data$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)

# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(unbalanced_original_data$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_unbalanced <- unbalanced_original_data[trainIndex, ]
test_unbalanced <- unbalanced_original_data[-trainIndex, ]

colnames(test_unbalanced) <- make.names(colnames(test_unbalanced))  # Make all column names valid
colnames(test_unbalanced) <- colnames(train_unbalanced)  # Align with the training dataset
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_1, newdata = test_unbalanced, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_unbalanced$stroke <- factor(test_unbalanced$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_unbalanced$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}

# Check Confusion Matrix Input
print(table(test_predictions_class, test_unbalanced$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_unbalanced$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      # No cross-validation
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_1 <- train(
  stroke ~ ., 
  data = train_unbalanced,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"


# Predict probabilities on the training set
train_predictions_prob <- predict(logistic_model_1, newdata = train_unbalanced, type = "prob")
train_predictions_class <- ifelse(train_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert predictions to factors to match the training set
train_predictions_class <- factor(train_predictions_class, levels = levels(train_unbalanced$stroke))

# Confusion Matrix for Training Set
conf_matrix_train <- confusionMatrix(train_predictions_class, train_unbalanced$stroke, positive = "Yes")

# Extract Metrics from the Confusion Matrix
precision <- conf_matrix_train$byClass["Precision"]
recall <- conf_matrix_train$byClass["Recall"]
f1_score <- conf_matrix_train$byClass["F1"]
specificity <- conf_matrix_train$byClass["Specificity"]
accuracy <- conf_matrix_train$overall["Accuracy"]

# Generate a Classification Report for the Training Set
classification_report_train <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)

```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_1, newdata = test_unbalanced, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_unbalanced$stroke <- factor(test_unbalanced$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_unbalanced$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}

# Check Confusion Matrix Input
print(table(test_predictions_class, test_unbalanced$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_unbalanced$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_unbalanced$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Step 1: Mapping function for comprehensive variable names
rename_variables <- function(variable) {
  case_when(
    variable == "age" ~ "Age",
    variable == "work_typechildren1" ~ "Work Type: Children",
    variable == "work_typeSelf.employed1" ~ "Work Type: Self-Employed",
    variable == "hypertension1" ~ "Hypertension",
    variable == "smoking_statussmokes1" ~ "Smoking Status: Smokes",
    variable == "work_typeGovt_job1" ~ "Work Type: Government Job",
    variable == "heart_disease1" ~ "Heart Disease",
    variable == "avg_glucose_level" ~ "Average Glucose Level",
    variable == "ever_married1" ~ "Ever Married",
    variable == "smoking_statusformerly.smoked1" ~ "Smoking Status: Formerly Smoked",
    variable == "gender1" ~ "Gender: Female",
    variable == "Residence_type1" ~ "Residence Type: Urban",
    TRUE ~ variable
  )
}


# Step 2: Define a function to generate the importance table for a specific model
generate_importance_table <- function(model, model_name = "Logistic Model") {
  # Extract coefficients
  coefficients <- summary(model)$coefficients
  
  # Create the variable importance table
  variable_importance <- as.data.frame(coefficients) %>%
    rownames_to_column("Predictor") %>%             # Convert row names to a column
    mutate(
      Predictor = rename_variables(Predictor),      # Rename variables
      `Odds Ratio` = exp(Estimate)                 # Calculate Odds Ratios
    ) %>%
    select(Predictor, Estimate, `Odds Ratio`, `Pr(>|z|)`) %>%  # Keep relevant columns
    rename(
      Coefficient = Estimate,                      # Rename Estimate to Coefficient
      `P-value` = `Pr(>|z|)`                       # Rename P-value column
    ) %>%
    filter(Predictor != "(Intercept)") %>%           # Exclude the intercept
    arrange(desc(`Odds Ratio`))                # Sort by absolute value of coefficients
  
  # Display the table with a dynamic caption
  kable(variable_importance, caption = paste(model_name, "- Predictor Importance with Odds Ratios")) %>%
    kable_styling(full_width = FALSE, position = "center")
}
generate_importance_table(logistic_model_1, "Logistic Model 1")
```

### Results: 2.	Balanced Original Dataset

#### Train-test split

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Load the dataset
balanced_original_data <- scaled_stroke_tb

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
balanced_original_data[cat_variables] <- lapply(balanced_original_data[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
balanced_original_data$stroke <- factor(
  balanced_original_data$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)

# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(balanced_original_data$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_balanced <- balanced_original_data[trainIndex, ]
test_balanced <- balanced_original_data[-trainIndex, ]
```

#### Model

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      # No cross-validation
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_2 <- train(
  stroke ~ ., 
  data = train_balanced,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the training set
train_predictions_prob <- predict(logistic_model_2, newdata = train_balanced, type = "prob")
train_predictions_class <- ifelse(train_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert predictions to factors to match the training set
train_predictions_class <- factor(train_predictions_class, levels = levels(train_balanced$stroke))

# Confusion Matrix for Training Set
conf_matrix_train <- confusionMatrix(train_predictions_class, train_balanced$stroke, positive = "Yes")

# Extract Metrics from the Confusion Matrix
precision <- conf_matrix_train$byClass["Precision"]
recall <- conf_matrix_train$byClass["Recall"]
f1_score <- conf_matrix_train$byClass["F1"]
specificity <- conf_matrix_train$byClass["Specificity"]
accuracy <- conf_matrix_train$overall["Accuracy"]

# Generate a Classification Report for the Training Set
classification_report_train <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)
```

Improved Metrics
```
	•	Accuracy: 77.45% - Strikes a better balance between positive and negative classes.
	•	Recall (Positive Class): 81.35% - Effectively identifies most true positives.
	•	F1-Score: 78.29% - Indicates a good balance between precision and recall.
	•	Specificity: 73.5% - Lower, suggesting some false positives.
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_2, newdata = test_balanced, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_balanced$stroke <- factor(test_balanced$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_balanced$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}

# Check Confusion Matrix Input
print(table(test_predictions_class, test_balanced$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_balanced$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_balanced$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

generate_importance_table(logistic_model_2, "Logistic Model 2")
```

### Results: 3. Low-Risk Age Group

Objective: Build the first stratified model for the low-risk age group.
	
Approach
	•	Dataset Split: Divide the original dataset into training and test sets.
	•	Preprocessing: Apply previously mentioned preprocessing steps to the training set.

Next Steps
	•	Train a logistic regression model on the low-risk age group training data.
	•	Evaluate model performance on the test set.

### Train-test split

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
low_risk_age_tb[cat_variables] <- lapply(low_risk_age_tb[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
low_risk_age_tb$stroke <- factor(
  low_risk_age_tb$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)


# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(low_risk_age_tb$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_low_risk <- low_risk_age_tb[trainIndex, ]
test_low_risk <- low_risk_age_tb[-trainIndex, ]

# Balance the training set 
train_low_risk <- upSample(
  x = train_low_risk[, names(train_low_risk) != "stroke"], 
  y = train_low_risk$stroke, 
  yname = "stroke"
) 

# Scale both training and test set
train_low_risk <- train_low_risk %>%
  mutate(across(where(is.double), ~ standardize(.)))

test_low_risk <- test_low_risk %>%
  mutate(across(where(is.double), ~ standardize(.)))
```

### Model

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_3 <- train(
  stroke ~ ., 
  data = train_low_risk,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the training set
train_predictions_prob <- predict(logistic_model_3, newdata = train_low_risk, type = "prob")
train_predictions_class <- ifelse(train_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert predictions to factors to match the training set
train_predictions_class <- factor(train_predictions_class, levels = levels(train_low_risk$stroke))

# Confusion Matrix for Training Set
conf_matrix_train <- confusionMatrix(train_predictions_class, train_low_risk$stroke, positive = "Yes")

# Extract Metrics from the Confusion Matrix
precision <- conf_matrix_train$byClass["Precision"]
recall <- conf_matrix_train$byClass["Recall"]
f1_score <- conf_matrix_train$byClass["F1"]
specificity <- conf_matrix_train$byClass["Specificity"]
accuracy <- conf_matrix_train$overall["Accuracy"]

# Generate a Classification Report for the Training Set
classification_report_train <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)

```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_3, newdata = test_low_risk, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_low_risk$stroke <- factor(test_low_risk$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_low_risk$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}

# Check Confusion Matrix Input
print(table(test_predictions_class, test_low_risk$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_low_risk$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```
probably due to several factors:
- different stroke distribution in the training set
- overfitting to the training set structure
- the small size of the test set

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_low_risk$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```


```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

generate_importance_table(logistic_model_3, "Logistic Model 3")
```

### Results: 4. High-Risk Age Group

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"


# Load the dataset
high_risk_age_data <- scaled_high_risk_age_tb

# Ensure valid factor levels for all binary and categorical variables
cat_variables <- c("gender", "hypertension", "heart_disease", "ever_married",
                   "Residence_type", "smoking_statusformerly.smoked", 
                   "smoking_statussmokes", "work_typeGovt_job", "work_typeSelf.employed", "stroke")

# Explicitly set factor levels to "0" and "1"
high_risk_age_data[cat_variables] <- lapply(high_risk_age_data[cat_variables], function(col) {
  if (!is.factor(col)) col <- factor(col)  # Convert to factor if not already
  levels(col) <- c("0", "1")  # Ensure levels are consistently "0" and "1"
  return(col)
})

# Ensure the target variable "stroke" has valid levels
high_risk_age_data$stroke <- factor(
  high_risk_age_data$stroke,
  levels = c("0", "1"),         # Original levels
  labels = c("No", "Yes")       # Valid names for levels
)

# Split into training (80%) and testing (20%) sets
set.seed(42)  
trainIndex <- createDataPartition(high_risk_age_data$stroke, p = 0.8, 
                                  list = FALSE, times = 1)

train_high_risk <- high_risk_age_data[trainIndex, ]
test_high_risk <- high_risk_age_data[-trainIndex, ]
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Custom summary function for Precision, Recall, and ROC AUC
custom_summary <- function(data, lev = NULL, model = NULL) {
  precision <- tryCatch(posPredValue(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  recall <- tryCatch(sensitivity(data$pred, data$obs, positive = lev[1]), error = function(e) NA)
  roc_auc <- tryCatch(pROC::auc(pROC::roc(data$obs, data[, lev[1]])), error = function(e) NA)
  c(Precision = precision, Recall = recall, ROC = roc_auc)
}

# Train logistic regression model
ctrl <- trainControl(
  method = "none",      
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = custom_summary # Use custom summary function
)

# Train logistic regression model on the training dataset
set.seed(42)
logistic_model_4 <- train(
  stroke ~ ., 
  data = train_high_risk,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)
```

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the training set
train_predictions_prob <- predict(logistic_model_4, newdata = train_high_risk, type = "prob")
train_predictions_class <- ifelse(train_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert predictions to factors to match the training set
train_predictions_class <- factor(train_predictions_class, levels = levels(train_high_risk$stroke))

# Confusion Matrix for Training Set
conf_matrix_train <- confusionMatrix(train_predictions_class, train_high_risk$stroke, positive = "Yes")

# Extract Metrics from the Confusion Matrix
precision <- conf_matrix_train$byClass["Precision"]
recall <- conf_matrix_train$byClass["Recall"]
f1_score <- conf_matrix_train$byClass["F1"]
specificity <- conf_matrix_train$byClass["Specificity"]
accuracy <- conf_matrix_train$overall["Accuracy"]

# Generate a Classification Report for the Training Set
classification_report_train <- data.frame(
  Metric = c("Precision", "Recall", "F1-Score", "Specificity", "Accuracy"),
  Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print the Classification Report
print(classification_report_train)

# Print the Confusion Matrix for Context
print(conf_matrix_train)

```

Performance Metrics
```
	•	Accuracy: 63.70% - Moderate performance overall.
	•	Recall (Positive Class): 66.11% - Captures a fair portion of true positives.
	•	Precision: 63.07% - About 63% of predicted positives are correct.
	•	F1-Score: 64.56% - Reasonable balance between precision and recall.
	•	Specificity: 61.30% - Low, leading to higher false-positive rates.
```

Intetpretation
```
	•	Strength: Moderate ability to capture true positives.
	•	Weakness: Struggles to distinguish between classes, with low specificity and accuracy.
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"

# Predict probabilities on the test set
test_predictions_prob <- predict(logistic_model_4, newdata = test_high_risk, type = "prob")

# Ensure the correct column is used for the positive class
colnames(test_predictions_prob)  # Check column names to confirm

# Convert probabilities to binary predictions
test_predictions_class <- ifelse(test_predictions_prob[, "Yes"] > 0.5, "Yes", "No")

# Convert to a factor with consistent levels
test_predictions_class <- factor(test_predictions_class, levels = c("No", "Yes"))

# Ensure test_unbalanced$stroke is also a factor
test_high_risk$stroke <- factor(test_high_risk$stroke, levels = c("No", "Yes"))

# Validate Levels of Predictions and Actuals
if (!all(levels(test_predictions_class) == levels(test_high_risk$stroke))) {
  stop("Mismatch in factor levels between predictions and actuals!")
}
  
# Check Confusion Matrix Input
print(table(test_predictions_class, test_high_risk$stroke))

# Generate Confusion Matrix
conf_matrix_test <- confusionMatrix(
  data = test_predictions_class, 
  reference = test_high_risk$stroke, 
  positive = "Yes"
)

# Correct Metric Extraction to Handle NA
precision <- ifelse(!is.na(conf_matrix_test$byClass["Precision"]), conf_matrix_test$byClass["Precision"], 0)
recall <- ifelse(!is.na(conf_matrix_test$byClass["Recall"]), conf_matrix_test$byClass["Recall"], 0)
f1_score <- ifelse(!is.na(conf_matrix_test$byClass["F1"]), conf_matrix_test$byClass["F1"], 0)
specificity <- conf_matrix_test$byClass["Specificity"]
accuracy <- conf_matrix_test$overall["Accuracy"]

# Generate Metrics Report
classification_report_test <- data.frame(
    Value = c(precision, recall, f1_score, specificity, accuracy)
)

# Print Results
print(conf_matrix_test)
print(classification_report_test)
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
# ROC Curve
roc_curve <- roc(test_high_risk$stroke, test_predictions_prob[, "Yes"], levels = c("No", "Yes"))

# Plot the ROC curve with AUC
auc_value <- auc(roc_curve)
ggplot(data = data.frame(
  TPR = roc_curve$sensitivities,
  FPR = 1 - roc_curve$specificities
), aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_value, 3)), color = "black") +
  theme_minimal()
```

```{r, message = FALSE}
#| code-fold: true
#| code-summary: "Click to show code"
generate_importance_table(logistic_model_4, "Logistic Model 4")
```


## Conclusion

## Grading criteria:

Criteria	(Max Points 20):	Description

Content	(4):	Clear and comprehensive explanation of the project, its goals, and outcomes.

Organization	(4):	Logical flow of information, from introduction to conclusion.

Teamwork	(4):	Demonstration of collaboration and presentation dynamics (individual projects automatically earn full points).

Visuals	(4):	Well-designed and relevant slides or visual aids that effectively convey information.

Presentation Mechanics	(4):	Clear speaking, good video/audio quality, and adherence to time limits.

## Requirements:

Introduction: Briefly introduce your project goals and motivation.

Research Questions: What were the key research questions?

Data: Provide a high-level overview of your data sources and preprocessing.

Findings and Results: Highlight your key findings, insights, or model results.

Conclusion: Summarize your main takeaways and future potential directions.

Length: 7 minutes or less. The time limit will be strictly enforced. Content:

Tell your story well! Focus on delivering clear, concise, and engaging insights from your project. 🚀

## Remarks for presentation slides:

Lets not show results in the presentation that are not relevant. Aim remains to predict stroke.